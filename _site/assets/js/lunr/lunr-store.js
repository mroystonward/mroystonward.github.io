var store = [{
        "title": "Lustfaust",
        "excerpt":"An exercise in fictional archives and mediated histories - focussing upon little-known, 1970s, German noise band, Lustfaust.                                                                                                                                                                                         Exhibitions and Performances: ICA, London (April 2006); Freight &amp; Volume, New York (July 2006); Haunch of Venison, Berlin (September 2007); ICA, London (July 2008); Big Chill, Ledbury (August 2008), Rampa del Lingotto, Turin (November 2008); Museo Madre, Naples (June 2009); Tatton Park, Knutsford (May 2010); Fishmarket Gallery, Northampton (July 2010); Grand Union, Birmingham (September 2010); Teatro Eliseo, Rome (Feb 2011) Creators/Contributors/Collaborators: Alex Williamson, dandelion + burdock, Euan Rodger, Jamie Shovlin, Laura McLean-Ferris, Lillevan, Mike Harte, Murray Royston-Ward, Patrick Farmer, Peacho, Schneider TM &amp; Kptmichigan, Sigrid Holmwood, Stephen Linehan. Archived Website ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/lustfaust/",
        "teaser":"http://localhost:4000/assets/images/portfolio/lustfaust-001.jpg"},{
        "title": "When We Hit the Sky We Were High Over the Roofs, a Field of Gnarled Antennae",
        "excerpt":"Audio installation activating listeners through dynamically modulating sound in space.                                                                                               ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/when-we-hit-the-sky/",
        "teaser":"http://localhost:4000/assets/images/portfolio/hit-the-sky-001.jpg"},{
        "title": "The Rusty Trombone of God",
        "excerpt":"Cardiff based DIY gig collective - bringing fringe highlights and dirt-rare performances to Cardiff between 2010 and 2012. And then Nottingham in 2013. Some-time collaborators: The Chameleon, Joy Collective, JT Soars, Mr. Ian Peebles, Sŵn Festival, Ian Watson.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Artists: Akke Phallus Cartography, Ashtray Navigations, ASSS, Bear Man, Belied Gunaiko, Birchall/Cheetham Duo, Chuck Johnson, Circles of Don Boxeo, Deas + Denton, H Hawkline, Hard Pan Trio, His Naked Torso, Jauge, Lutto Lento, Mars to Stay, Meddicine, Nacht und Nebel, Natural Snow Buildings, Nazka, Nick Jonah Davis, Our Love Will Destroy The World, PartWildHorsesManeOnBothSides, Piotr Kurek, Primordial Undermind, R. Stevie Moore, Rattle, Roman Nose, Sneaky Earnest, Sounding, Surfacing, Team Sports, Tepeu y Q’uq’umatz, Thought Forms, Thurston’s Excommunicated Mole Duo, Tropical Ooze, Ultrahumanitarian, Was Ist Das, Vampire Blues and Yajé.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/rtog/",
        "teaser":"http://localhost:4000/assets/images/portfolio/rtog-001.jpg"},{
        "title": "A Walk by the Water",
        "excerpt":"Between the 25th and the 30th of September 2013, in groups and alone, we walked sections of the River Trent and Nottingham Canal. It was as if she knew we were coming as she had put on a great symphony.                                                                                                                                                           Score for City Mapping Sit by a river Use the materials around you Imagine you are somewhere else Imagine you are here Sound your territory Go home ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/walk-by-the-water/",
        "teaser":"http://localhost:4000/assets/images/portfolio/water-001.jpg"},{
        "title": "Contact Mics Uganda",
        "excerpt":"A 2 day workshop ran in the Katwe slums of Nsambya in Kampala, Uganda during December 2013.                                                                                               Participants built contact mics learning basic soldering. We also branched out into wiring audio cables and other connectors. We then went out into the community with our mics and an audio recorder documenting and intervening as we went. The participants learned how to use recording equipment and how to interrogate their environment from new perspectives. In the final stage of the project we mixed our recordings to create a sound collage as a kind of postcard exploring identity and environment. Installation Audio (excerpt)","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/contact-mics-uganda/",
        "teaser":"http://localhost:4000/assets/images/portfolio/contact-mics-uganda-001.jpg"},{
        "title": "Audio Transducers Nepal",
        "excerpt":"In March 2014 I visited Nepal with my wife. I was there to run a workshop where we would turn objects in speakers allowing us to intervene locally with sounds. As it was exam time in mainstream schools I was placed in a deaf school and the workshop needed to be rapidly adapted.                                                                                               The circuitry amplifies and transforms a sound signal (such as from an mp3 player or mobile phone) to drive a piezo disc contact mic to vibrate. Attaching the piezo to a resonant object amplifies these vibrations further, in space, like a speaker. These speakers aren’t clear sound reproducers but take on resonant characteristics of the objects themselves. The vibrating piezo disc can be felt and we were instead able to work with deaf students allowing them to touch the sounds directly and through surfaces. The students built the entire apparatus themselves from components which I provided and they learned basic soldering and circuit building skills alongside creative interaction with the world around them. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/audio-transducers-nepal/",
        "teaser":"http://localhost:4000/assets/images/portfolio/audio-transducers-nepal-001.jpg"},{
        "title": "Do You Want To Expand Your Parameters Or Play Museums Like Some Dilettante",
        "excerpt":"Composed in response to Marvin Gaye Chetwynd’s exhibition at Nottingham Contemporary as part of their ‘Memory’ project and permanent collection.                                                                 The score attempts to sonify the anonymous and largely voluntary labour required to animate the exhibition throughout its run. Download Score ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/expand-your-parameters/",
        "teaser":"http://localhost:4000/assets/images/portfolio/expand-your-parameters-001.jpg"},{
        "title": "The Memory Project",
        "excerpt":"A realisation of the score ‘Do You Want To Expand Your Parameters Or Play Museums Like Some Dilettante’ is played back using vibrations to activate the window as a speaker.                                                                                                 “Do you want to expand your parameters or play museums like some dilettante?” are provocative words spoken to Lou Reed by Andy Warhol in ‘Work’ from ‘Songs for Drella’. Lou had been somewhat lazy in writing songs and Andy is expressing his Catholic work ethic.   Warhol’s Factory industrialised artistic production and attempted to dissolve the hierarchies of such labour. Despite this it was impossible for the work to escape the gravitational pull off the ‘art star’.   Recently art again attempts to play with collaboration, authorship, improvisation and non-hierarchical strategies though again it is rare for such work to escape it’s association with an artist’s name who carries cultural or economic weight.   I am interested in the art workers whose names don’t appear in vinyl lettering, who aren’t recognised in the blurb and who don’t feature on the website.   The installation by Marvin Gaye Chetwynd features the labour of a revolving cast of performers whose role is to bring the works to life for an audience.   This intervention attempts to make such labour audible and tangible, making it recognisable to the public and collectable by the institution.   A range of bands, most infamously the band ‘Lightning Bolt’, eschew the stage and play in the middle of the crowd. Born out of necessity such practice becomes a vital critique of the hierarchies of performance.   Placed on the floor, barely audible and in a transient space we return to the questionable assertion that “It’s work, the most important thing is work.” Installation Audio (excerpt)The Memory Project presents the work of students of MA Fine Art (Nottingham Trent University). During the Tala Madani and Marvin Gaye Chetwynd exhibitions, each participant transformed their own practice into an indirect form of documentation. The works act as a means of remembering these recently-closed shows. Whilst our galleries are in changeover, they will occupy subtle and incidental places within the building, before forming part of the archive of Nottingham Contemporary, contributing to the memory of its programme of temporary exhibitions and events. Participating artists - Christiane Bressolier, Louise Creuzeau, Karen Hazelton, Katja Hock (MA Course Leader), Murray Royston-Ward and Susanne Thoene. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/the-memory-project/",
        "teaser":"http://localhost:4000/assets/images/portfolio/the-memory-project-001.jpg"},{
        "title": "Rainforest IV",
        "excerpt":"Based upon David Tudor’s Rainforest IV (1973) and realised as a collaborative electronics workshop and performance. Between 1968 and 1973 David Tudor developed ‘Rainforest’ which he describes as ‘Sounds electronically derived from the resonant characteristics of physical materials.’ The fourth (and final) iteration was concerned with hanging large ‘sculptural’ objects and making them sound in space. Rather than using commercial transducers we built DIY amplifier and contact mic systems, based upon explorations into the music making process implicit in the building of sonic technologies, to achieve the same aims. Ran as a workshop over 2 days we built the transducers and arranged the space, culminating on 30.06.14 in a 2 hour performance. Performers/Workshop Members:   Ben Hallatt - Kay Hill, Kiks/Gfr  Kelly Jayne Jones - Part Wild Horses Mane on Both Sides, Belied Gunaiko  Murray Royston-Ward - The Lows and the Highs Records  Ian Philip Watson - Swefn, Phantomhead TapesFilming and photo credits: Horatiu Gheorge                                                                                               ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/rainforest-iv/",
        "teaser":"http://localhost:4000/assets/images/portfolio/rainforest-iv-001.jpg"},{
        "title": "Optophonic Workshop",
        "excerpt":"I was invited to run a workshop as part of PS2N’s Optophonic Workshop.                                                                                               We built the same amplifier box and contact mic with transformer combination as used in Rainforest IV. This was a development of the Transducer workshop as originally conceived for Nepal. Participants learned soldering and circuit building skills as well as getting to take home some audio hardware that they could use with other skills and equipment encountered throughout the days activities. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/optophonic-workshop/",
        "teaser":"http://localhost:4000/assets/images/portfolio/optophonic-workshop-001.jpg"},{
        "title": "They Ain’t Fucked with the Milk since Louis Pasteur, Except Maybe Diluted It a Little Bit",
        "excerpt":"Sound installation and score drawing upon the alternative tuning systems of La Monte Young and the musical legacy of Sunny Murray.                                                                                               Triangle waves based upon La Monte Young’s ‘Well Tuned Piano’ interact in space creating harmonic fluctuations around the room. Inspired by Sunny Murray’s cymbal techniques these harmonics are set up in the space to be punctuated by percussive accents. The harmonic frequencies are chosen to create the possibility of an ecstatic response, channeling the emotional powers activated through Free Jazz and Fluxus era composition. The environment directly implicates the listener’s physical presence with tones shifting in relation to one’s movements and position. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/aint-fucked-with-the-milk/",
        "teaser":"http://localhost:4000/assets/images/portfolio/aint-fucked-with-the-milk-004.jpg"},{
        "title": "Improvisations: Series 1",
        "excerpt":"An ongoing archive of staged realities and material flows leaking into the world, inviting you to listen with the ‘ear of the imagination’.                                                                                               ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/improvisations/",
        "teaser":"http://localhost:4000/assets/images/portfolio/improvs1-001.jpg"},{
        "title": "FOAM",
        "excerpt":"One off 12” vinyl commission for Mat Jenner’s FOAM project.                                                                                               For FOAM I chose specifically to respond to the physical aspects of the record. For side A I modified a turn-table to feedback the sounds picked up by the stylus, back into the record itself via a physically vibrating audio transducer. Manipulating the record playback and controlling the feedback system became a performative exercise exploring the record as an object. For side B I worked with micro-tonal composition and various stereo panning effects to project frequencies into space. These frequencies activate the listening space and implicate the listener, though their position and/or movements, within the the sound generating process. The sonic processes explored, such as heavily panning sub-bass, are notoriously difficult to effectively cut into vinyl. As such this was an intentional problematisation of the vinyl record by directly inviting the possibility of the failure of the medium to effectively reproduce the material.   FOAM is a peripatetic and ongoing artwork by artist Mat Jenner conceived as an enquiry into the contemporary condition of the art object and its relationship to spectatorship, dissemination and display. At the heart of FOAM is an archive of commissioned, 12” dub plate records, made and exhibited within strict conceptual parameters.   ‘Records are unique and not duplicated. They can only be listened to in situ within a physical location and are not broadcasted or digitally disseminated’   Selected artists are asked to respond to these parameters in the form of a one-off record for insertion into the growing collective body of the archive.   FOAM’s attempt at ‘resistant distribution’ aims to privilege the material qualities of each record and confers the status of art object to a media form that is, by its nature, dispersed and mass produced. the effect is to ground commissioned art works to a specific time and place, generating an insistence on a direct, physical relationship between audiences and recorded artworks.   FOAM’s format is blurred and it’s authorship indistinct. A formal reading of FOAM’s artistic proposition can simultaneously occupy different positions - encompassing expansive artwork, contemporary archive, group exhibition, curatorial frame, installation, commissioning platform, performative stage and social space. For its presentation at the attic, an additional artistic protagonist has been introduced with the aim of further blurring foam’s form and it’s multitude of authorship positions. Artist Calvin Sangster has been commissioned to design and build a new installation and structure that responds to jenner’s formal, aesthetic and conceptual brief. Sangster’s contribution sets out to provide a future structural direction for the archive and its continued evolution as it tours, and expands in size.   As part of this expansion the attic has invited artists Alice Gale–Feeny, Jenna Finch, Sebastian Jefford, Finbar and Dexter Prior, Simon Raven and Murray Royston-Ward to make new works for inclusion in the archive. These works form the basis of the next 100 artists and mark the next step in the archives development as a collective object.   During the presentation of FOAM, the ATTIC has commissioned performances by Larry Achiampong, David Blandy and Marie d’Elbee to use the space built by Sangster. Further events will be scheduled throughout the duration of the show. To mark FOAM at the attic a special limited edition 7” record by Seb Patane will be launched and available for purchase here. http://onethoresbystreet.org/portfolio/158/ ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/foam/",
        "teaser":"http://localhost:4000/assets/images/portfolio/foam-004.jpg"},{
        "title": "Rammel Club",
        "excerpt":"Nights of visionary global music in Nottingham, UK. From 2014-2015 I joined Nottingham based DIY gig collective Rammel Club.                                                                                                                                                                                                                                                                                                                 Artists: [D-C], A.N.T. Attack, Arianne Churchman, Bir, C Joynes, Cathy Heyden/Rogier Smal, Colossloth, Core of the Coalman, Crank Sturgeon, Dale Cornish, Daniel Voigt, David Birchall + Javier Saso, Dusted Magazine, Evil Moisture, Experimental Sonic Machines, Father Murphy, Flamingo Creatures, Friend of a Friend, Fritz Welch, H.U.M, Ian Watson, John Macedo, Kay Hill, KK Null &amp; Kawabata Makoto, Lovely Honkey, Mai Mai Mai, Mammothwing, Marlo Eggplant, Melanie O’dubshlaine, Mesa of the Lost Women feat. Junko, Murray Royston-Ward, Nacht und Nebel, Nadir, Nick Jonah Davis, Nick Jonah Davis + Jo Cormack, Olivier Di Placido/Fritz Welch Duo, Pain Jerk, Papal Bull, Phantom Chips, Phil Julian, Posset, Rainbow Grave, Roman Nose, Rudolf Eb.er, Russell Haswell, Simon Raven, Simon Wildfrid, Smut, Sophie Cooper, Stuart Chalmers, Surfacing, Tepeu y Q’uq’umatz, the Sons of Rest, Thin Raft, Trans/Human, Was Ist Das? DJ, Woven Skull, Yerba Mansa ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/rammel-club/",
        "teaser":"http://localhost:4000/assets/images/portfolio/rammel-001.gif"},{
        "title": "Reactor Halls E14: Cables - Contact Mic Workshop",
        "excerpt":"Workshop conducted as part of the Rammel Club/Reactor Halls E14: Cables event.                                                                                               There is something inherently musical in the process of actually building and using audio technology. We tend to view music as the artefacts left behind (scores, recordings etc.) rather than as a complex of participatory social activities (playing intruments, listening, dancing etc.) Through workshops I aim to facilitate an exploration of an expanded rubric of improvised ‘musicking’ which incorporates the making and use of such technologies. For ‘Cables’ I will be drawing upon recent workshops in Uganda, Nepal, Sheffield and Nottingham; offering a drop-in surgery for the humble ‘contact mic’. Materials and tools will be available to build and experiment with contact mics so come along and get involved. Drop in/out surgery for contact mics. From building and improving through to different ways of using them for recording or live work. A selection of materials were available for soldering and testing contact microphones along with various instruments and objects for exploring their possibilities. Passers-by were invited to take part in an informal setting with many then going on to use contact mics as part of the ‘ANT Attack’ Noise Jam workshop. Reactor Halls (Wayback Machine Link as site doesn’t support HTTPS) ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/contact-mic-surgery/",
        "teaser":"http://localhost:4000/assets/images/portfolio/contact-mic-surgery-001.jpg"},{
        "title": "Bitto Arts Trust: Gasworks Residency",
        "excerpt":"A Triangle Network Fellowship at Britto Arts Trust in Dhaka, Bangladesh in 2015                                                                                                                                                                                         Exploring improvised ‘musicking’ and socially engaged arts practices I made a number of improvised performances and recordings. I presented this work on national television and worked with Dhaka Electronic Scene to develop workshops, performance and investigations into ‘narrative’ soundworks. I also undertook citizen noise level readings as part of a critical investigation into ‘noise’ and its cultural interconnections. Much of this work is collected in this artist’s book - My Neighbor Who Lives in the City of Mirrors near My House                                                                                                                                                           As part of the residency I also connected with the local arts community by running a series of open forma and collaborative ‘Listening Groups’ with the intent of encouraging philosophical and personal discourses around sound and music - both in a Fine Arts context and where they intermingle with ‘popular music’. All listening group materials are archived here: https://mroystonward.gitlab.io/Britto-Listening-Group/ ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/britto/",
        "teaser":"http://localhost:4000/assets/images/portfolio/britto-001.jpg"},{
        "title": "UNIT(e) 2016",
        "excerpt":"10 week UNIT(e) Residency at g39 in Cardiff.                                                                                                                                                                                         For UNIT(e) I wanted to take the installations I’d previously been working with out of the gallery and into the community. Portable and battery powered. I experimented with circuitry to connect memories and spaces from previous projects to new locales and situations through temporary and portable sound installations. g39 (Wayback Machine Link as site doesn’t support HTTPS) ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/unite/",
        "teaser":"http://localhost:4000/assets/images/portfolio/unite-001.jpg"},{
        "title": "rfm",
        "excerpt":"Introduction I’ve just wasted 5 days on something that didn’t work. That’s not entirely uncommon, but what’s most frustrating is that this task should have been easy, and I didn’t see this failure coming at all. All is not lost, there are possible solutions on the horizon. I should also point out that, this was a project designed to help demonstrate how easy some of the p2p web stuff, I’ve recently been talking about, actually is (https://mroystonward.github.io/an-opinionated-critical-guide-to-diy-digital-publishing/). Sod’s bloomin law! Trying to salvage something from this though I’d like to talk about when things go wrong, and they frequently do. In particular, mostly with helping relatives and friends with technical things, there can be a sense of fear to experiment, to poke and prod, with technology. Mostly this can seem to be a fear of breaking things, and I wonder if this goes back to early experiences when learning to use software etc. In the context of the music community I associate with, where improvisation and experimentation often features, I’d like to make the claim that similar processes equally apply to software, computers, coding, servers etc. I’ve never had any training in any of this but, through trial and error, have gradually built up a patchy but functional knowledge base. Of course, there’s not enough hours in the day and frequently competing priorities but, should you be inclined, things going wrong is part of the process. The Original Proposal Seeing as I’ve been talking about the merits of using peer-to-peer (p2p) technologies for DIY publishing, I thought I’d try a practical demonstration that people could either try themselves or repurpose for their own ends. I was going to mirror and re-publish Radio Free Midwich using Beaker Browser (which uses the DAT protocol). I really wanted to do this because a) the site is excellent (though sadly dormant), b) it acts as a bit of a rallying call around the No-Audience Underground, and c) it feels appropriate to help maintain archival copies in free and open ways. I messaged Rob Hayler and got his blessing to proceed. Below is what happened next. Downloading a Site Mirror with wget First off I needed to scrape the site, in this case radiofreemidwich.wordpress.com. I did a quick search online and it looked like ‘wget’ would be the best tool to use. It’s a tool I’ve used before but never delved into so I found a few examples online for the parameters to use. Mostly I followed the instructions here. The command I ran was: $ wget -mk -w 20 radiofreemidwich.wordpress.comjust to explain the options in this command:   m - creates a mirror (and is shorthand for several useful wget commands).  k - converts links, once it’s finished downloading the site, so they work relatively and off-line.  w - waits, in this case 20 secs, between download requests. This eases off the burden placed on the server you’re requesting from. For wordpress probably not a big deal but it’s good etiquette anyway.The First Seeds of Things Not Going to Plan Actually before I ran the above command I tried a slightly different version. I wasn’t convinced that I’d done it correctly so tried the ‘proper’ way. My trial run however has suggested this would take a few hours so I left it running, plugged in, and make a hot drink. I wasn’t prepared for the fact that the -w 20 parameter would make this whole process take 4+ days. Ouch!!!! I wont change my advice but in the future I’d weigh up removing ‘-w 20’ part. I’d say, consider the nature of the site you are downloading and how much strain you are likely to put on the servers. For hosted wordpress we likely won’t tax them too much, but for a small, limited site we could temporarily slow it down to a grinding halt. The other thing that wasn’t very satisfactory is the nature of the downloaded site. On the one hand wget has done its job properly but the results are highly inefficient. The wordpress site has many, many posts and they are collected in a number of different ways. The main site landing pages display posts in chronological order, secondly there are numerous ‘tag’ pages, ‘category’ pages and even some searches that are somewhere/somehow embedded within the site. When wget scrapes the site, it scrapes all of those multiple representations of the individual posts, so a single post may be saved under it’s date, along with multiple copies for each of a number of tags, plus trackbacks, and comments. This results in the ~630 posts becoming ~3-4k duplicates and different representations. What in reality probably only uses approx 150 MB of diskspace now takes up 550-600 MB. What that all basically means, is that I’m wasting time, space, and resources, on a highly inefficient representation of the rfm site. Beaker browser The next stage in my project was to set-up the site using beaker browser. I wrote a tutorial to install and set this up. As this is still useful information I leave this, unedited, below. Feel free to skip or still try for yourself. On the whole Beaker works brilliantly and the problem I later encounter is a little specialised, being fixed, and shouldn’t trouble you at this stage. Installing/Exploring Beaker Browser In the meantime you can also install Beaker Browser. Download and follow instructions from here. Or, if you’re like me and use Homebrew on a Mac then run: $ brew update$ brew cask install beaker-browserLet’s launch beaker and have a quick look around. Beaker works like any other web browser but it’s also capable of accessing sites using the dat:// protocol. Addresses look like long, random, complex strings but a) this will be fine, and b) there’s ways around this that we’ll explore in another post. If you open the settings page you can see where Beaker defaults to for projects (in my case the ‘Sites’ directory).         Beaker Settings Page  You probably want it to be the default for dat:// links. You could use Beaker as your main browser too but I’m sticking to Firefox, for several plugins I use with it, so http:// is off. Next, open the ‘Library’ page. In my example I’ve already gotten two projects set up. You’ll likely have nothing. If you click the blue ‘New’ button we can create a new p2p archive/project.         Beaker Library Page          Creating a new project in Beaker  You can create a blank project and put stuff in it later. Or a basic website which has a html index page you can edit.         Beaker Blank Website Project  I’ll show you this a bit more because it’s kinda cool. You get a list of files and you can edit these files directly in the browser. You’ll need some basic HTML chops as there’s no WYSIWIG interface etc. but it’s really easy to work with. In the following images you can see that I simply edited the index.html page by adding ‘Blah Blah Blah’ to the existing template.         Editing index.html          Updating the text of index.html          Here’s the edited webpage  You can add and import files/folders so you could build something quite complex if needed.         Adding a new file/folder to the project  Right, I’m going to scrap that.         Moving the project to the trash          Permanently deleting the project  Creating our Site Mirror in Beaker Browser So, with Beaker set-up I went on to create a new project using an existing folder (our downloaded RFM mirror). Then Beaker crashed. I restarted it and my project was still there, so I didn’t worry. I ‘published’ the site and Beaker ground to a halt. I left it struggling for an hour but it couldn’t cope and I had to force quit it. Restarting Beaker, it froze almost instantly, and I had to restart and close the tabs quickly, before it froze, to get enough functionality back, to delete the project and get Beaker back to normal. So, I tried it all again and went through the same problems. Here’s one reason why opensource projects are great. I searched the public issues here - https://github.com/beakerbrowser/beaker/issues - and found a few cases of people having similar problems with large files or directories. The developers had noted that there were some inefficiencies in the DAT protocol and Beaker browser that were being addressed. In fact, these issues were old enough that they should have been fixed by recent beta releases. I checked the version of beaker I was running and I was up-to-date and in fact more recent that the version which should have solved these problems. Damn, still no answer. Next I raised an issue here - https://github.com/beakerbrowser/beaker/issues/1169. Within an hour I’d gotten a response from the deveopers. Basically, there’s more work to do here but further improvements should roll out soon. Conclusions Where does this all leave me? Well, waiting for a fix. I can try a different way of getting the RFM site though. I can rebuild it from an XML export. This involves more work on my part and doesn’t replicate the visual design of the site so that’s even more work. I could publish a different way but that defeats my purpose of demonstrating Beaker. In this instance I will see how I get on with the XML export but won’t commit any major time to it whilst I focus on other priorities. In the meantime, I’ll see if my issues gets updated. It’s rare to not have any other workarounds though, so if this was time-sensitive or critical there’d usually be options. What I think is the biggest take-away here, and why I still wanted to publish this even though it failed, is that it demonstrates the process of failing, improvising, and trying alternatives. I don’t have the coding skills to work on Beaker browser myself, or write my own software, or do any number of other complicated things. But I can still participate and contribute by getting my hands a bit mucky. Basically, don’t worry about fucking this shit up if you fancy having a go, and there’s always sources of help/information when it does go wrong. A Final Positive Note I’ve published with Beaker Browser before so this is definitely only an issue with a large, inefficient site. My other experiments have worked without hitch. If you want to see some sites hosted using Beaker/DAT try these:   dat://thehouseorgan.hashbase.io / dat://aab4cc712ee92808b1311eed724fc9fa0ace5dc0e40e242e3f9ec2f2f26ee587/ / https://thehouseorgan.hashbase.io  dat://time-stretch.hashbase.io / dat://fa38bb17fc5039ded8363e40541306d6e285f1b6b983c1029f32b33e1dd44d5e/ / https://time-stretch.hashbase.io","categories": [],
        "tags": [],
        "url": "http://localhost:4000/rfm/",
        "teaser":"http://localhost:4000/assets/images/diy-digital-500x300.png"},{
        "title": "Hello World",
        "excerpt":"Obligatory first post. Portfolio and more to follow. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/welcome-to-jekyll/",
        "teaser":"http://localhost:4000/assets/images/teaser500x300.jpg"},{
        "title": "Self Hosting with Raspberry Pi and Tor",
        "excerpt":"A year or so ago I started experimenting with digital self publishing. Having become increasingly politicised with regard to both state and commercial online surveillance I also started questioning the role centralised platforms such as soundcloud and bandcamp play in independent music distribution. I’ll write more on this later — on motivations and failures in my critique/experience — but for now I wanted to talk about how I set up a Raspberry Pi webserver at home for publishing a Tor hidden service. My own project, The House Organ, is available on the ‘clearweb’ at https://thehouseorgan.xyz. I’ve recently moved it from github to gitlab pages and setup a custom domain for it. I’ve also maintained a Tor Hidden Service mirror of this site at tho2f4fceyghjl6s.onion/. The idea behind this is that the site would be censorship resistant and allow for visitors to connect with a degree of anonymity (I doubt that these are very high risks for me personally but there is a political spectrum here that I want to be engaged with). If you want to know more about what Tor is and how it works https://www.torproject.org/about/overview.html.en is a good place to start. I’ll save discussion of why one would want to enagge with any of this for a later date To set up the Tor mirror I have setup a small webserver at home with the site proxied through Tor - all running on a Raspberry Pi. What follows are notes, self reminders and tips for trying this yourself. I made numerous mistakes on the way so feel free to ask if you encounter any problems, but at the same time this is a rather rough guide and not a step by step tutorial so treat it like a recipe that may need alterations to suit your tastes. Setting up a Raspberry Pi A few notes on this:   We’re setting up in a basic headless server configuration so we’re not attaching a monitor and keyboard to interact with the Raspberry Pi. Instead we’ll login via the command line and execute commands. I found that this all took a bit of getting used to but has been useful training for working with servers in general.  For the above reasons I’d install Raspbian over NOOBS. Technically, seeing as NOOBS contains Raspbian, you should be able to do all of this with either, but you don’t need the extra bloat so I wouldn’t bother.  I’ve only done this with Raspberry Pi 2s and 3s. I don’t know if it’d work with a Pi zero and an older original Pi model might struggle a little bit if you run other services on it.  Whenever I’ve done this I’ve generally stuck to Ethernet so haven’t had to faff around much with WiFi. The transfer speeds on your local network are going to impact site load speeds over the Internet also so if you can use Ethernet do, but if not don’t worry about it.  Technically, we’re setting up a web server which probably violates your home ISP terms of service. If you started attracting much traffic your ISP might look at this more closely. Theoretically you could bump up to a business plan if needed but please just be aware of any risks you are placing upon yourself.  As you’re setting up a mini-web-server at home and exposing services to the outside world you should take precautions to ensure these don’t get hacked etc. Setting up passwordless login is a very good idea. It sounds like a bad idea but it relies on a cryptographic key for login so it’s a lot harder to hack than a password is. Setting up a firewall is essential and only expose the ports/services needed. Software such as fail2ban is worth implementing too, so as to block any suspicious activity.So, caveats and notes out of the way I’d recommend this guide - RRaspberry Pi 2: Basic setup without any cables (headless) (Wayback Machine Link as site doesn’t support HTTPS) - is easy to follow and covers basic security such as setting up a firewall and passwordless login. It’s likely getting a bit old now so do check the official Raspberry Pi guides etc. too. Installing Tor First some more caveats. We’re using Tor to serve a webpage as a hidden service. To access these hidden services visitors need to use Tor browser. Tor browser helps with anonymity and privacy but if your use case/threat model is higher then you’ll need to look into this more deeply and make habitual changes. Also, we’re running the server from home so whilst we’re hosting a ‘hidden service’ we’re not doing it in an anonymous manner. Tor has an undeserved reputation for unsavoury and illegal activities but if you were wanting to host anything remotely illegal I don’t think hosting a hidden service at home would be a very sensible way to do it. Also, on a final note, I’m based in the UK where there aren’t currently any legal restrictions on using Tor, your local situation might differ. Right, let’s get Tor installed. We need to add Tor’s package repositories. This will open a new file for us to paste in the links. nano /etc/apt/sources.list.d/torproject.listWhen I set up Raspbian it was on version Jessie so I added: deb http://deb.torproject.org/torproject.org jessie maindeb-src http://deb.torproject.org/torproject.org jessie mainIf you’re on stretch etc. you can get the package details here - https://www.torproject.org/docs/debian.html.en#ubuntu ctrl+x to exit and save nano. Now run these 2 commands to set-up gpg keys to validate these sources. gpg --keyserver keys.gnupg.net --recv A3C4F0F979CAA22CDBA8F512EE8CBC9E886DDD89gpg --export A3C4F0F979CAA22CDBA8F512EE8CBC9E886DDD89 | sudo apt-key add -Next we need to update the package manager: sudo apt-get updateAnd install Tor: sudo apt-get install torSetting up a Hidden Service Now we need to configure some settings in Tor. The config file is called ‘torrc’ and is located at /etc/tor/torrc sudo nano /etc/tor/torrcUnderneath where it says: ############### This section is just for location-hidden services ###You need to add (or uncomment/edit) the lines: HiddenServiceDir /var/lib/tor/hidden_service/HiddenServicePort 80 127.0.0.1:7658The first line can be any folder writable by Tor. The second line can be unpicked a bit more: HiddenServicePort 80 means that Tor will proxy to this virtual port. You could change it but I can’t think of many use cases where you’d need to so I’d stick with the standard configuration. 127.0.0.1 specifies proxying internal connections. When we set up the web server this means it will proxy the sites we set up through Tor. :7658 This is the internal port we’re proxying. My web server is set to use this port. You can change this but your web server will need setting up to be the same as whatever you choose here. ctrl+x to exit and save. Now restart Tor sudo service tor restartProviding everything worked and Tor restarted it will have created some files in the /var/lib/tor/hidden_service/ folder. private_key is the private key pair and you should back this up and shouldn’t share it with anyone. You shouldn’t need to interact with this file though. hostname is your onion address, e.g. duskgytldkxiuqc6.onion. This is the address you’ll put into Tor browser to visit your site. The hidden_service folder is pretty locked down security wise (which is sensible) so: sudo cat /var/lib/tor/hidden_service/hostnameTo display the onion address. Setting up a Webserver There’s more than one way of doing this. Tor doesn’t recommend Apache as it’s a very large, complex software with a lot of potential for misconfiguration and security leaks. In our use case - a home based DIY Raspberry Pi server - I’m not sure how significant this is. Still, I opted for lighttpd as it’s fairly simple and lightweight. sudo apt-get install lighttpdI made some additional folders to structure multiple sites as follows. Replace foobar with whatever makes sense for your site name etc. sudo mkdir /var/www/htmlsudo mkdir /var/www/html/foobarNow we change the owner and permissions of the folder and add pi to the www-data group. sudo chown -R www-data:www-data /var/www/html/foobarsudo chmod 775 /var/www/html/foobarsudo usermod -a -G www-data piNow put your website files into the folder /var/www/html/foobar If you don’t have any files for this yet you could make a basic index.html page: echo '&lt;h1&gt;Hello, World!&lt;/h1&gt;' &gt; /var/www/html/foobar/index.phpNow we need to edit the lighttpd config file: sudo nano /etc/lighttpd/lighttpd.confchange server.document-root to (or whatever folder structure you chose): server.document-root        = \"/var/www/html\"and change server.port to the following (or whatever port you chose in the torrc config above): server.port                 = 7658Now we’re going to add our website as a virtual host. The main reason is so that we can add other sites later if we want to. Add the following lines: $HTTP[\"host\"] == \"duskgytldkxiuqc6.onion\" {      server.document-root = \"/var/www/html/foobar\"}duskgytldkxiuqc6.onion is whatever onion address you got earlier. server.document-root should point to whichever folder you created earlier and put your web files in. now restart lighttpd: sudo service lighttpd restartOpening Ports and Testing it All Everything should now be setup but our firewall should be blocking access so we need to open up a hole in ufw. sudo ufw allow 7658This should be it, the site doesn’t need to be accessible any way other than via Tor so no other ports should need opening (though you probably opened up port 22 in the Raspberry Pi setup so as you can SSH. Make sure you keep this port open or you won’t be able to login to your Pi). Lighttpd is serving your site locally but it isn’t accessible externally. Tor however is proxying this site and serving it as a hidden onion service. Visiting duskgytldkxiuqc6.onion (or whatever your equivalent is) in Tor browser from another computer should display your website. And that’s it really. I hope it works for you as it did for me. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/self-hosting-with-raspberry-pi-and-tor/",
        "teaser":"http://localhost:4000/assets/images/high-tor-500x300.jpg"},{
        "title": "Urghh... Recaptcha",
        "excerpt":"Just a quick note. I hate captcha. Mostly I hate infinite captcha loops you can get locked into whilst browsing using Tor. That’s often as much to do with cloudflare than captchas themselves but still… I also hate that we’re basically feeding google’s AI machine learning for free when we complete a captcha. They commercialise our browsing labour for developing the kinds of algorithms that are increasingly driving society and, in the process, driving/maintaining inequalities. Google aren’t the only ones here of course (and they’re not the only captcha system either). Smash capitalism innit :punch: 🏴 Anyway. I set up comments here the otherday using staticman. I opted not to enable captcha because I thought I’d see how things went and try and weather the bots if and when they came. Well 48 hrs in on a super obscure little corner of web publishing and I’m already getting daily spam comments. Stuff that! So apologies to anyone who wants to comment and shares my captcha hatred but I’ve enabled google’s reCaptcha so you’re going to have to endure clicking some cars or street signs etc. Peace out :v: ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/urghh-recaptcha/",
        "teaser":"http://localhost:4000/assets/images/hero-recaptcha-demo-500x300.gif"},{
        "title": "EOTK: Tor Mirroring on Google Cloud Engine",
        "excerpt":"So at the risk of this site mostly being tech stuff I’m posting another how-to, rough-guide, self-documentation regarding Tor. I hadn’t planned this as I’m currently a) meant to be completing a PhD funding application, b) should be trying to find a job and c) trying to focus on music related things rather than tech for a little while. Anyway,  @moananddrone got me intrigued by the Enterprise Onion Toolkit (EOTK) https://github.com/alecmuffett/eotk as he’s got a project requiring it and wanted me to help look at it with him. I won’t share any details of the project as Kevin will be talking about that at some point, in the future, as appropriate. Still I wanted to share my own experiences of trying out this tool and deploying it using Google’s cloud infrastructure. There are political considerations in using Google’s (or Amazon’s etc.) cloud platforms and I’m not agnostic towards such politics. I also want to keep myself educated about the various technologies in use so this seemed like a good opportunity to test out EOTK, have a play with Google’s free introduction to their cloud services, and do all of this separately to my existing Tor installation (which works well as is — I didn’t want to risk breaking my configuration with another toolset). Google Cloud Platform I’m not really documenting this part of the process very extensively as they’re just visual web forms which are fairly self explanatory to complete and this would just be a series of screenshots which I don’t want to do. If anyone reading struggles here though and the help pages don’t answer your questions feel free to comment and ask questions. First thing was to signup to Google Cloud Platform’s free trail - https://cloud.google.com/ I then created a new project and a Compute Engine VM. I’m not 100% sure what I’m doing here but I’m trying only a micro instance with Ubuntu 16.04. I also opted for European data centres simply because European data privacy laws are generally speaking considered better that US ones (a complex and contentious issue but for my current needs this simplification is appropriate). I’m not familiar with the firewall settings here either so I’ve enabled HTTP and HTTPS. We’ll see if I need to do more config later. The online SSH client is truly impressive. Just click the SSH button/link and you get a web based terminal. Installing EOTK First things first: sudo apt-get updatesudo apt-get dist-upgradeNow to install EOTK git clone https://github.com/alecmuffett/eotk.gitcd eotk./opt.d/install-everything-on-ubuntu-16.04.shHopefully everything is now installed. Setting up an EOTK Project Lets try it, still in the eotk folder we can run: ./eotk genThis generates an onion key file, in my case u35unrkwyvkcdxsl.key. Copy the name of this, we’ll need it in a moment. Let’s set up a configuration file. I’m going to call it mroystonward.conf but you should call it whatever you like but keep the .conf at the end. nano mroystonward.confIn the nano text editor we need to write the following: set project mroystonwardhardmap secrets.d/u35unrkwyvkcdxsl.key mroystonward.github.ioA few notes here. mroystonward is the name of my project. Use here whatever you used previously. secrets.d/ is the folder where eotk puts onion keys so this needs to stay the same. u35unrkwyvkcdxsl.key should be replaced with whatever onion key you generated above. mroystonward.github.io is the domain I’m mirroring via Tor. You should put whatever domain you want to mirror. A few notes here though. Don’t put the www hostname. You can do quite complex subdomain setups which I just don’t need in this example. Check the docs and video - here and here Firing it up Now run (replacing the name of the conf file with whatever you chose earlier): ./eotk conf mroystonward.confand (with whatever project name you chose earlier): ./eotk start mroystonwardThat’s it, it’s running. We can check the status using: ./eotk status mroystonwardAnd you should see both Tor and Nginx processes listed. In Tor Browser you can now visit your onion address (mine is u35unrkwyvkcdxsl.onion). Like me you probably now encounter some errors and warnings.         Your connection is not secure  SSL Certificate Problems This is expected behaviour and it’s because the SSL certificates are self signed. You’ll need to click ‘advanced’ and add an exception. You may have to do this several times. There’s not a huge amount we can do about this either. Certbot/LetsEncrypt doesn’t currently support .onion domains so getting free/simple SSL isn’t an option. You currently need an SSL EV certificate which, i.e. from DigiCert, costs ~$250. For non-corporate projects this is a) expensive and b) I’m led to believe is hard to get without the proofs that accompany corporate infrastructure/incorporation etc. Depending on the nature of the site HTTPS may not be entirely necessary. This is a new area for my investigations into Tor etc. My current understanding is that the layers of encryption provided by Tor preclude the need for HTTPS on security grounds. I may be wrong here though. Feel free to comment. A HTTPS certificate does however help with proof of ownership and trust, even in an onion environment. Useful resources in starting to consider the pros and cons of HTTPS over Tor:   https://www.eff.org/deeplinks/2012/03/https-and-tor-working-together-protect-your-privacy-and-security-online  https://www.eff.org/pages/tor-and-https  https://blog.torproject.org/facebook-hidden-services-and-https-certsWhat these resources don’t make sufficiently clear is the different ways one may choose to use Tor. You may use the Tor browser for increased privacy/anonymity whilst browsing clear-net websites (i.e. *.com, *.org, *.net, *.co.uk etc.). The EFF graphic describing the different levels of protection Tor and HTTPS together offer is referring to this kind of browsing. This is one reason the Tor browser includes (and benefits from) the HTTPS Everywhere plugin. You may also use the Tor browser to access .onion sites (as we’re setting up here). That this relies upon secure keys provides similar protections to HTTPS. You also know that only one site has this key and so when you connect to an onion domain it is almost certainly the site you think it is. You might not know who owns the site and they may not have your best interests at heart but it’s not anyone else pretending to be that site or intercepting traffic etc. Still, we now generally understand that when the browser gives us a security warning about certificates we should take this seriously. Encouraging users to just accept self-signed certificates isn’t good practice either. Trying HTTP Only If we don’t need HTTPS we can add this line to the mroystonward.conf file: set force_https 0You’ll need to stop, rerun configuration and then start your project again: ./eotk stop mroystonward./eotk conf mroystonward.conf./eotk start mroystonwardWhilst this is useful to note it doesn’t work for me as this entails downgrading from HTTPS to HTTP. But Don’t Downgrade from HTTPS Whilst I’m not yet sure of my position regarding the benefits of HTTPS over Tor for a fairly simple static website with little in the way of logins or cookies, if the original site is already HTTPS, downgrading would be considered bad practice and if there were any secure cookies and or data submission on the site we would now potentially be introducing all sorts of security holes. The EOTK developers are looking into this but for now there isn’t really any elegant solution. Tor are also considering how the Tor browser should handle onion sites with self-signed certificates. Added to this, I host mroystonward.github.io using github pages. A nice feature of this is that they enforce HTTPS. That unfortunately means that even if I set force_https to 0, the part of the process which involves github serving the clear-web site to the EOTK process is all enforced in HTTPS. That means that I’m trying to downgrade to HTTP which isn’t recommended or supported as above. There are inelegant ways around this but none that are available to me as I can’t alter how github serves it’s pages. Startup Scripts I also faffed around for quite sometime with startup scripts via the compute engine metadata flags but I could never get everything running properly. To save you time don’t bother. Luckily EOTK comes with a handy init script tool. Run the following commands (in the eotk folder still): ./eotk make-init-scriptsudo cp eotk-init.sh /etc/init.dsudo update-rc.d eotk-init.sh defaultsThat’s it, it’ll now start all projects on boot. Parting Thoughts It all seems very slow to me so far (and slower than a site I directly run as a Tor service from a Raspberry Pi). UPDATE: Things seem a lot faster now so my speed concerns seem mostly unfounded. I know, for example, images on the github site aren’t very optimised as a compromise for the simplicity of the static site publishing process (something I’m looking at fixing at some point). I scaled up the cloud VM to see if that made a difference and too be honest I don’t think it has so I’m sticking with the cheaper micro option for now. So, interesting exercise. If you fancy trusting my self-signed certificate you can see this in action at u35unrkwyvkcdxsl.onion. I’ll leave it running for a while and see what happens but I doubt it’ll be up for a long-long time. I’m also impressed by how easy this was to set up on a cloud platform. It suggests the possibilities for temporary set-up/use/tear-down approaches as needs fit. One could easily clone the git repo and add in some default conf files and automate the initialisation process. Also, the options to use a small cluster of RaspberryPis and OnionBalance for all of this is super intriguing too but I don’t yet have a use case for it to justify getting more Pis - https://www.youtube.com/watch?v=HNJaMNVCb-U Hope this helps you have a play around too. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/eotk-tor-mirroring-on-google-cloud-engine/",
        "teaser":"http://localhost:4000/assets/images/onions-500x300.jpg"},{
        "title": "Why Being Disconnected from Practice Isn't Really a Thing",
        "excerpt":"One hears people, especially in arts circles, talking about their relationship to practice. Artists and other creatives (not forgetting nurses, teachers and well beyond) don’t just think up ideas and they magically appear, let alone the idea that it even starts with an ‘idea’ rather than a more embedded and entwined being in the world (more on that another day). Artists do stuff and make things. Practice. Professionally though this can be a bit of stick to beat yourself with, especially if your practice isn’t very ‘visible’ or has few tangible outcomes. On a residency a while back there were several participants whose primary motivation was related to ‘reconnecting’ with their practice. But what does that actually mean? I can’t speak for others but I can speak for myself. Two years ago, following a string of post-masters projects, workshops and residencies, I lost momentum. Myself and my partner managed a little travel between my last two engagements (both residencies) and relocated to London (where we still live). During the relocation financial pressures played a significant role. I was also looking for a bit of a new challenge and started working in education. I’ll talk more about that in a moment but basically, life got in the way and I’ve done very little visible creative work. During ‘arting’ and ‘workshopping’ thoughts around pedagogy arose. It was hard to get a grip on what this actually meant to me and I felt an urge to work in education as a starting point. A range of personal experiences, and a desire to engage critically and ethically with them, pushed me towards ‘challenging’ students and so I ended up working in a PRU (Pupil Referral Unit) with socially and emotionally challenged teens. We had some ASD, various levels of SEND, frequent verbal abuse, numerous safe-guarding concerns, minor drugs issues, a bit of violence and a fair whack of property damage - all alongside some truly amazing young adults, laughs, crazy stories, amazing progress and I even got to run a little bit if a music production workshop. The first year was tough but rewarding, the second year mostly just tough. I won’t labor trying to explain this beyond what I saw as poor practice from some entrenched staff members; and a downward spiral of bureaucracy. The wider political climate is truly awful also. Not to let these factors off of the hook but also every year will be different and inevitably some things will or won’t gel better than other years. This last year wasn’t my year and it took a toll on me. Both the work demands and the emotional labor — all of which go well beyond a ‘working day’. The pay’s shit too (and I had it better than others)! I struggled to maintain recording music during this time, either collaboratively (The Sons of David Ginola’s release was from jams preceding this last year) or solo (I have recorded nothing in nearly 2 years — well except for a started but not finished possible project with yol which I failed to get off the ground in the summer holidays). My only performances of the past 2 years were Extraction Music and Electric Knife - both when we first moved. I did manage to finally sort out a range of long standing printing issues and belatedly put out some artist books, but without maintaining any kind of visible practice I may as well have been pissing into the wind. That must sound like a depressing, self-pitying, ‘negging’ though, and it is, but it also isn’t. It is because I’d be lying if I said I didn’t feel some of those things. I was mostly thinking this during emotionally gruelling days where it all rubs off and gets the better of you. It isn’t though because there’s no actual yardstick by which we’re meant to measure ourselves as an indicator of success. What we actually mean when we say we’ve become disconnected from practice is that our practice has become disconnected from the systems of value and exchange frequently necessary to try and sustain practice. Often these systems are commercial and market based and the idea that our self-worth as practitioners is determined by such forces is awful and something we should actively resist. This is why I think our frequent beliefs around ‘practice’ can become a stick to self flagellate with. In my case the practice wasn’t very visible but it did exist. I have been experimenting with privacy and cryptography aware DIY digital self publishing. I have also developed a range of skills, critical understandings, ethics and approaches to teaching (whether formal or informal) — I’ve developed a pedagogy (this and the digital publishing are more interrelated than it might seem on the surface too). This is positive, this is practice, it just doesn’t look like what we’d like it to look like when we begrudge becoming disconnected from our practice. There are certainly ways to make this practice more visible, there are ways of transforming it, but those ways aren’t always practical or suit the work. We can always reflect and try better but we’re still practicing when we’re figuring this stuff out. The next challenge for me then is to try and integrate these threads of my practice and to figure out ways to make them more visible (because at some point we probably do want to connect with audiences and even the market aspects of creative practice). One step that seems important in all this is the archive. We could, but we won’t, delve very deeply into the politics and meanings of archives, rather I’d suggest that most of us try and archive our own work. Whether that be through keeping copies, scrapbooks, portfolios, meticulous documentation, backups etc. The other thing about our archives is that despite our best efforts they aren’t stable. They’re always shaped by our changing relationships to our archives. This is fairly well understood and feels pedestrian to say again but it seems to be like a good place to start to reconnect with practice (though what I really mean is to connect aspects of practice that stray from the widely accepted models of how practice should function in an arts economy and make them more visible with the ultimate aim of maintaining some kind of professional practice that may attract benefits, opportunities, kudos and payment). I’m leaving it there. Not much of a conclusion. Rather, I feel it’s more important to critically engage with what ‘being connected with practice’ really means and begin to find ways of embedding it into our praxis. The archive may or may not work for you, it may not work for me, it is where I’m starting though and I wanted to discuss why publicly. Other than that, I’ll be sharing more stuff as I go. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/why-being-disconnected-from-practice-isnt-really-a-thing/",
        "teaser":"http://localhost:4000/assets/images/reconnecting-with-practice500x300.jpg"},{
        "title": "Testing Epoxy Backed Contact Mic's",
        "excerpt":"I’ve been making contact mic’s for myself and others for some years now (and I’m planning on putting some up for sale again quite soon so get in touch if you want any making to order). Whilst they’re very simple to make and utilise fairly cheap parts there’s a handful of variables and build options at play. There are some expensive high quality one’s on the market (Jez Riley French’s spring to mind) but I’ve always been more about durability. I want a contact mic that will take a good bashing. (This has worked well for a number of people where, e.g. they have been used to mic up a log which was being hacked at with a toy axe made from frozen piss.) I’ve tried a few variations here, especially in terms of how it is wired and where on the mic the wire is positioned. These variations, over time, have ultimately proved to be less successful than a standard wiring directly onto the piezo element. What has worked consistently well though is backing the element/wiring with epoxy resin. Often twists and snaps of the wiring are the primary site of failure and so encasing these joints in glue helps secure it. It also has the bonus of protecting the element from random shorts whilst handling etc. I want to do some comparisons with the high-end mic’s on the market at a later date (I need to buy some first) but in the meantime I thought I’d check what effects this epoxy backing actually has on sound quality/mic performance. My test was fairly simple. Two mics made from the same components (wiring, piezo elements etc.), one epoxy backed and the other bare. Here’s a picture of the set-up.         A/B Contact Mic Testing in Window with Vibration Speaker  I thought about the sound source and decided to use something pre-recorded and chose a sound collage I’d made in Iceland with a combination of piano abuse and field recordings. I thought there was a broad tonal range to help determine if there were any significant difference across the spectrum. I played this collage back using a vibration speaker placed on a wooden window bay. It was placed fairly centrally and the contact mics were an equal distance either side of the speaker. I thought the material qualities of the wood might bring out some of the bottom end and, seeing as contact mics generally don’t perform well in this part of the spectrum, I thought it would make a useful comparison. Both mic’s went into a Zoom H4n with the gain set equally. I noted at the recording stage that the epoxy backed mic was recording at a lower volume so it does already look like the resin effects sensitivity of the mic. Once recorded I imported the stereo file into Audacity. I normalised the tracks (I chose not to maintain stereo balance so L/R signal were essentially normalised independently of one another). I then split the stereo track into separate mono files. Here are those normalised tracks for you to compare for yourself. I listened back and also used Ableton Live’s built in spectral analysis. I’m a complete lay person here but have concluded the following. I couldn’t spot any discernible difference in frequency response. There are obviously minor differences as the 2 tracks do sound slightly different. I couldn’t however isolate any areas of the audio spectrum where the epoxy backed mic performed worse than the unbacked mic. As noted above, the backed mic was slightly less sensitive. This means that when it was normalised the noise floor would have been amplified slightly too. Usually this wouldn’t present a significant issue as we’d simply raise the sensitivity of the recorder. I couldn’t do this for the purpose of testing but, if I was setting the recorder to the appropriate level for the mic, normalisation wouldn’t have increased the volume as much or any background preamp noise. As to the qualitative differences. Well, I can’t quite put my finger on it. I think the higher and lower ends perform very similarly. Instead, I have a theory. I think there are some resonances in the unbacked recording that aren’t very strong in the epoxy-backed recording. I think that the piezo element has natural, material resonances itself. These elements are initially designed to act as buzzers so there is a natural frequency at which they will oscillate and self-resonate. When the piezo element is distorted by vibrations it generates an electrical signal and this is what we hear when we record it and play back through a loudspeaker. I think that these self-resonances, the material properties of the piezo element, are shaping the recording slightly and that this is the difference we hear. The epoxy resin backing suppresses the distortion of the piezo element. This makes it a little less sensitive to vibration but also suppresses the resonant properties of the piezo element. Practically speaking my interpretation is that the epoxy backed contact mic is a little quieter when recording so needs a little more gain. It performs equally well across the frequency spectrum (within the general limitations of a contact mic’s frequency response). The material resonances of the piezo element are somewhat suppressed. What the suppression of those resonances actually means is quite subjective. Suggesting one is more ‘honest’ is quite ridiculous as any recording/listening process is far from ‘neutral’. For general purpose I think I prefer less of those resonances. For others they may be preferable. I could certainly see making work specifically addressing those resonances. The purpose of this experiment though was to understand what difference epoxy backing made and to determine whether it had particularly negative or unfavourable consequences. I feel I can confidently say they perform as well if not better with the caveats of personal preference/context regarding resonance. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/epoxy-backed-contact-mics/",
        "teaser":"http://localhost:4000/assets/images/contact-ab-500x300.jpg"},{
        "title": "Onionised Bandcamp",
        "excerpt":"I’ll cut to the chase, I’ve ‘Onionised’ Bandcamp. If you want to see it in action it’s here bandc2v6rbqrn6vx.onion (You need the Tor browser to visit .onion links - https://www.torproject.org/) A word of warning though, you need to jump through some hoops due to HTTPS over Tor. If you know what you’re doing check out this ‘landing’ page - https://gist.github.com/mroystonward/aec1fa678186fe62c692b1d5b9d129c2 / bctorowb7blvvryo.onion - if not, read on.         Bandcamp Onion Address in Tor Browser  Platforms, Surveilance and Commercialisation of Data   Independent artists, punk labels, and do-it-yourself (DIY) organizers have long suggested that the means through which music is created and distributed carries as much political weight as the content of the songs—by subverting the status quo, making their own platforms, and creating alternative worlds.   What do these ideologies mean in today’s hyper-mediated, corporate digital culture? New enemies of agency now surround music communities in the form of centralized and exploitative platforms that musicians and listeners increasingly grow beholden to. (https://shadowproof.com/2017/11/01/protest-platforms-resonate-streaming-co-op-agency-to-artists/) Many platforms are exploiting your ‘content’ for corporate, commercial advantage. Driving engagement to maximise advertising revenues served alongside your work. Tracking and harvesting massive troves of data to train AI algorithms which can later be commercialised further or just selling it on via data-brokers. The political/societal ramifications spread wider, just consider the dynamics of power and capital associated with Facebook and Cambridge Analytica’s role in Brexit/Trump victories (consider the various investigative links here - https://en.wikipedia.org/wiki/Cambridge_Analytica#Criticism). The terms are frequently inequitable for artists (i.e. Spotify streaming payment rates) yet there seem to be few viable alternatives for engaging with audiences and the successful platforms have monopolised their spaces with little room for small independent publishing. Rather, the only way to ‘compete’ is to go in and ‘disrupt’ with rounds of angel backing to try and build something even bigger (though this pessimism is somewhat disingenuous as I do believe that collectively we can build better systems). Some of the worst excesses of such platforms seem to be thankfully absent from Bandcamp. They’re not profiting from advertising alongside your content and instead it’s in their interest to drive your sales for their business model. They’re not claiming undue license over the work you put online through their platform. Their fees certainly don’t seem unreasonable. PWYL embodies a tension along a continuum of exploitative benefits to the platform from music moving around itself easily and freely vs. an artistic desire to make sounds available widely and to try and engender forms of sharing and ‘patronage’ rather than a pure sales focus. There are options and consent (though sometimes requiring a premium subscription for maximum control over one’s own work within the limited options given by the platform). Bandcamp does however use Quantcast tracking and Google Analytics. I won’t labour this other than to say that Google Analytics, for most average users, is probably not of concern (however there should be broader public debate on the implications of state level surveillance programs and their access to such data), whereas Quantcast provide machine learning, demographic tracking and shopping insights etc. (https://www.quantcast.com/, https://en.wikipedia.org/wiki/Quantcast). I don’t know ‘how’ Bandcamp is using this. It may be driving recommendations (which for many has tangible benefits to ‘us’ as well as ‘them’) or other insights for optimising the platform. It may be part of a much larger data gathering exercise that provides Bandcamp with additional revenue. Either way, our browser activity is being tracked and commercialised both as ‘artits’ and ‘fans’ and the equitability of this relationship is unclear. I don’t know about how and how much Bandcamp engages in forms of censorship. They presumably have to deal with copyright infringement, issues around free and hate speech, they may have to engage different filtering depending upon geopolitical requirements. I can’t say I’ve heard about blocking, banning, spam etc. on Bandcamp the way it pervades other platforms and networks. That this could be done though still enters into the debate I wish to raise.   Beyond a certain tipping point, platforms become difficult not to use. All this means that power, and a conservative form of power at that, is deeply involved in platforms within the human world. But I think it is also important to emphasize that the reason platforms become successful is partly because they enable you to do things; they are productive, generative. Those seeking to oppose a given platform—a technical one, like Microsoft, Google, or Apple, or a political one, like Neoliberalism, or an aesthetic one, like conservative genre boundaries or tuning systems—have to remember that. It implies that a purely negative or transgressive project will be unlikely to be able to seriously disrupt the functioning of existing malignant platforms. Only the building of new, better ones, will suffice. Or perhaps the re-engineering of those already in existence. (http://www.glass-bead.org/article/reengineering-hegemony/ - note Internet Archive link due to source not being available via HTTPS) I say all this because the desire to ‘onionise’ Bandcamp isn’t one of being anti-Bandcamp. Rather it’s one of engaging with broader debate and scrutiny around content/market/social platforms, especially as they relate to power dynamics in publishing and sharing creative content.         By aceebee from Camberley, UK (Just yellow paint) [CC BY-SA 2.0 (https://creativecommons.org/licenses/by-sa/2.0)], via Wikimedia Commons  The Onion Routing network If you don’t know much about Tor already you should have a look around https://www.torproject.org/about/torusers.html.en. Whilst it is talked about and useful in terms of resisting state level surveillance it has a longer history in resisting corporate surveillance (and an even longer history in secure military communication). One can use Tor to browse relatively anonymously (this claim comes with caveats about browsing habits and online behaviour), securely and privately. It is also relevant in terms of censorship circumvention and network resilience. This won’t be a technical primer about Tor and it’s benefits/risks. It is perfectly legal to use here in the UK but your local jurisdiction may differ. Along with the many positive uses of Tor it has been, and continues to be, used for illegal marketplaces. Take the usual precautions online regarding links, spam, information sharing etc. If you do want more information about online privacy/security and the wider issues check out some of the following resources:   https://tacticaltech.org/  https://www.eff.org/  https://www.openrightsgroup.org/EOTK I have been exploring this area for a while now, publishing via https://thehouseorgan.xyz/ / tho2f4fceyghjl6s.onion using a home Raspberry Pi powered server (https://mroystonward.github.io/self-hosting-with-raspberry-pi-and-tor/). Having been recommended to look at The Enterprise Onion ToolKit (EOTK) by @moananddrone I mirrored this site (https://mroystonward.github.io/eotk-tor-mirroring-on-google-cloud-engine/). I have also been volunteering with Radical Librarians Collective as there is an overlap in issues of privacy and working with tools such as Tor. Related to this work @moanandrone has, quite exceptionally and it will be interesting to see the longer-term impact of this project, gone on to ‘onionise’ the University of West London Repository and his documentation offers some valuable insight as to how issues of intellectual freedom and centralised platforms effect scholarly work - https://moananddrone.github.io/bag-of-onions. If you are interested in how to use EOTK yourself the above links should be helpful.         By Unknown or not provided  Retouched by Mmxx [Public domain], via Wikimedia Commons  Decentralisation and Alternatives Decentralisation continues to offer strong alternatives to current calcifications of power. Opportunities have spread far beyond p2p file-sharing and torrents. The potential demise of Soundcloud last year raised 2 particularly interesting responses.   Soundcloud is a good example of a project with modest ambition gaining traction with subcultures who then elevate it to a place of relevance. As that growth is recognized, the gears of homogeneity start to turn, and the intent shifts from being one of creating something interesting to creating something larger. https://jon-kyle.com/entries/2017-08-19-platform-death/ Jon Kyle and the DAT community offered a Soundcloud archive tool (to preserve your tracks should the service shutdown) that would be served and hosted using p2p. I made an archive here - dat://5525a6916dbed65eb9ea8d1fe7501cda094f937ec7b674337e038dc8427077fd/ (you need Beaker Browser to view). Projects like ZeroNet and OpenBazaar are also viable alternatives.   Many factions of the independent musical infrastructure have become closer and closer aligned with corporate interests and advertising supported journalistic models in order to keep their head just slightly above water, and are in danger of losing their distinction and identity as a result. Unless we construct an intervention, this curve towards the commercially viable and mediocre templates of the bland, centralized, content warehouses ultimately threatens to displace the role of music as the preeminent critical space of psychedelic possibility, experimentation and collective desire. https://medium.com/blockchannel/soundcrowd-tokenizing-collectivizing-soundcloud-5c4f60ed4961 Elsewhere Matt Dryhurst suggested tokenising Soundcloud using blockchain technology to engender decentralised, collective ownership. His article is well worth a read and it raises numerous salient points. I would add to my general enthusiasm towards his suggestions that the crypto/blockchain space has a number of inherent issues that need to be resolved. Not only has ‘mining’ proved incredibly resource inefficient there are fundamental ‘oracle’ problems in ‘smart contracts’ which undermine the decentralisation possibilities. And that’s ignoring the general hyperbolic bubble surrounding such technologies. Still as a model for possible collective ownership there’s a lot of potential. There’s no reason why similar considerations couldn’t also feature in the ongoing development of Bandcamp or other such marketplaces. These aren’t systems of intervention we can easily implement within the platform though. We either set up alternatives (set up our own p2p archives etc.) or we take over the platform (tokenising Soundcloud would likely take the form of ‘buying it out’ and distributing it equitably).         By Zeynel Cebeci (Own work) [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], via Wikimedia Commons  Bandcamp Onion For now we aren’t looking to immediately replace or change Bandcamp (or Soundcloud etc.). We are looking to ensure that, as a platform, it offers opportunities to publicise our music without sacrificing the broader privacy of ourselves or our fans. Some of these benefits could be garnered through simply accessing the existing site using Tor browser and, in combination with HTTPS, we would experience improved privacy from various forms of tracking. Onion Services however improve anonymity/privacy even further and so I’d like to see more platforms, such as Bandcamp, embracing these positively (much as Alec Muffett has done with Facebook). Onionising Bandcamp is a provocation: it aims to gauge interest in using such a platform in this manner; it hopes to further debate around the role platforms play in music publshing and consumption; and it intends to show Bandcamp their onionised site in action in the hope they would implement it officially. The biggest problem with this implementation is to do with the way browsers handle HTTPS certificates. The EOTK proxy generates self-signed certificates which generate warnings. .onion domains don’t require HTTPS as the layers of encryption already present in the Tor network infrastructure offer greater security. The only real use for HTTPS with a .onion domain would be in cases of public sites also offering onionised versions and demonstrating ownership. In such cases the site would procure a signed certificate for the original site and the onion version. As I don’t own Bandcamp and am simply proxying it through Tor I have no claim to ownership. If anything what I am doing is unauthorised. I can not, and would not, provide a signed certificate for HTTPS. The implication is that it is necessary to allow these security exceptions as you browse. Where images, CSS and JS etc are handled by various CDN domains it becomes necessary to allow several exceptions for the site to simply display properly. If Bandcamp implemented a .onion domain themselves, with an appropriate HTTPS certificate, this game of certificate whack-a-mole would become unnecessary. The main Bandcamp Onion is available at bandc2v6rbqrn6vx.onion. If you want the site to function correctly though I would recommend pre-authorising several domains as listed here (or bctorowb7blvvryo.onion) Performance may vary and the current implementation isn’t setup ‘yet’ for scale. I’ll update this page as necessary and if Bandcamp respond to the provocation. Update - 2018-02-16: Yesterday I tweeted about this experiment and in one of those tweets I thanked Alec Muffett for the EOTK tool. This led to an exchange which addressed issues of consent. I wanted to update this post to reflect this exchange and have chosen to do so as an addition to the end of the article. There was some positive and useful engagement from Alec who is deeply experienced in this field. It will certainly help to shape communication with Bandcamp. The language of consent and dating/flirting, suggestive of sexual conduct, was also used and Jacob Applebaum was specifically mentioned along with a project he, and others, were involved in called ‘onionflare’. I was unaware of this project before this exchange and only really knew of Applebaum in relation to publicity surrounding acts of sexual abuse. I am also aware of the harm that these actions, and other similar cases of abuse and misogyny, have caused. Not being part of the communities affected I don’t feel there’s anything I can add beyond general disgust in his conduct and solidarity with his victims. I don’t want to avoid the issue of consent in this context as it does raise ethical issues that should be part of the discussion. I myself alluded to this early on in the post, specifically in reference to Bandcamp generally acting consentingly in their approach to artist’s uploaded content. I also mentioned later on that I didn’t do this with permission and that it was likely to be considered ‘unauthorised’. I had considered therefore that Bandcamp may not view this provocation kindly and may even go as far as to send some kind of cease and desist. Picking up on the use of the word ‘provocation’ it may suggest causing agitation carrying negative connotations. I want to highlight the arts context from which I have come where such a provocation is frequently framed as a positive intervention to spark critical engagement (for further context: the department where I studied has been pioneering aspects of an arts based critical pedagogy, drawing from Freire and Illich, that frequently used ‘provocations’ in lieu of traditional lectures). The idea here then isn’t to provoke Bandcamp to piss them off, but to provoke some of the Bandcamp community into critical engagement. I feel that parts of the original post regarding not being ‘anti-Bandcamp’, and the quote regarding the futility of purely transgressive responses to platforms of power and hegemony, provide the necessary context for such a reading of provocation but perhaps it is worth spelling it out more clearly. There are important reasons why onionising sites in this manner would be problematic. Not least the trust model of accepting self-signed certificates would likely be damaging in a long-term outward trajectory. There are complex configurations of power inherent in large platforms but there are still sysadmins and technical apparatus being deployed to ensure system integrity for our benefit as much as the platform’s (and in many ways we make the platform so simple us/them dichotomies don’t do these issues justice). Maverick actions which undermine such systems efforts are also potentially damaging. If there was the interest and use-case for an onionised Bandcamp (a question I hope this project is attempting to explore) these considerations stack up to compelling reasons Bandcamp should consider implementing this officially. In the meantime though this could be considered a threat to Bandcamp’s system integrity. There is also a complex history of Tor, the unfairly monikered ‘Dark Web’, Hacktivism and criminal activity that means that it is in the wider community’s interest to tread lightly, respectfully and in friendship. This should also exist alongside personal empowerment to challenge large hegemonic power structures though. This is the main reason why I don’t see the link with ‘onionflare’. I’m in no way proposing forcibly onionising the web with this project, I’m proposing we raise the level of discussion around power and privacy in relation to DIY online publishing and the platforms which dominate these activities. I’m also doing this on a small scale as an individual. I’m not wielding the power of a community or organisation to try and force others into Tor adoption. Perhaps a bigger challenge is the way, through onionising Bandcamp, I have drawn the labour of others into this project. From the design, coding and assets of the website (not to mention the labour in maintaining and supporting it), to the artwork and music of the many, many artists using the platform. This is an aspect that interests me significantly and I’m glad of the opportunity to raise it. The ethics of plagiarism and unauthorised use of my own work have affected me personally in an arts project I was deeply involved with some years ago. I have also previously made work based upon the uncredited, volunteer labour present in a major exhibition of an internationally renowned artist. The nature of proxying a website via EOTK makes it hard to fathom the relationship this has to the use of licensed works. The onion service is simply a gateway to the originating website that exists on the Tor network. Your web request goes to the onionised site, this requests the content from the originating site and then serves it back to you. When we put our content on Bandcamp (or other platforms) we have little to no say about how it is transmitted over network infrastructure. We cannot stipulate that we only want our work transmitted over a specific protocol, or that our work is never transmitted over communication lines in a particular geo-location, or through a particular ISP. We can’t for example stipulate that our work is only transmitted over HTTPS and that it isn’t transmitted over communication lines in Manchester or owned by BT. Whilst it would be a nightmare to implement perhaps we should be asking for that kind of control! These issues are not entirely unrelated to those explored by Saga though I’m describing a more extreme network infrastructure idea than the original platform based one. Irrespective of such technical concerns, if you have music on Bandcamp it is now drawn into the frame of this project whether you like it or not. That may be acceptable in terms of not having or expecting control over the technical infrastructure of your work being served by Bandcamp but it isn’t entirely ethical of me either. It’s such grey areas which are at the heart of Pirate Bay claiming no wrong-doing as they’re not ‘hosting’ illegal content or that google/youtube probably provide access to more illegal content than any ‘pirate’ site. I really don’t know how I feel about all of this. If someone actively objected to the appearance of their work within this project I know I couldn’t achieve the level of granularity to exclude them and that makes me uncomfortable. I don’t know whether all of these aspects were what Alec was thinking about with his responses but I do hope my comments help ensure that relevant issues are adequately present in any ongoing debate this provocation may raise. Comments are open so it would be great to hear other opinions. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/onionised-bandcamp/",
        "teaser":"http://localhost:4000/assets/images/bc-tor-500x300.jpg"},{
        "title": "Testing Impedance Matching Contact Mic's",
        "excerpt":"I’ve been thinking about and comparing contact mic’s recently for a combination of reasons. Firstly I have some broad assumptions that I’ve never really tested and secondly I’m curious as to how the ones I make stack up against other offerings. What makes a difference in a contact mic and how much difference does it really make? Previously I’ve compared how much difference backing my contact mic’s with epoxy resin (to make them more durable) makes to their performance - https://mroystonward.github.io/epoxy-backed-contact-mics/. The piezo-electric elements used in building contact mic’s are very high impedance and so various methods of matching, buffering and pre-amplifying the signal are recommended. I decided to buy and test an Hosa MIT-129 Impedance Matching Transformer. I tested at work on a metal-clad table using some rulers and sellotape to generate sounds for recording. The mic’s were placed roughly equidistant from the sound source.         Impedance Matching A/B Contact Mic Testing with Twangers and Sellotape  The sounds are a procession of ruler ‘twanging’, scraping and sellotaping. An image of the set up is above and the sound recordings are below (you can download from soundcloud too for your own analysis if you like). I listened back and also used Ableton Live’s built in spectral analysis. I’m still a complete lay person here but have some conclusions.         Impedance Matching A/B Contact Mic Analysis  In the image above I’ve highlighted 3 areas on the spectral analysis with the following thoughts:   A. The unmatched input shows a peak of activity below 20Hz. I’m surprised and confused by this. Firstly this kind of response at low frequencies is unexpected with a basic contact mic with no buffering/matching/etc. Secondly, this is below general human hearing and has little to no impact on the perceivable audio performance. I’m not saying these frequencies are meaningless (they undoubtably are part of the signal and potentially influence the perceptible frequencies) but simply, in cases such as this, don’t appear to be significant. I expect that there are perfectly reasonable electrical-engineering reasons for this but I’m still rather surprised.  B. Without impedance matching the contact mic is fairly unresponsive around the 80-90Hz range. This is likely part of where contact mic’s are considered to perform weakly with low frequencies.  C. The unmatched mic has several weak points throughout the 200Hz-2.5kHz range. This again weakens the low to mid range response and robs the recording of some richness. Said richness is present in the impedance matched recording along with generally improved low-mid frequency response.Below is another screenshot from later on in the track and again we see similar results.         Impedance Matching A/B Contact Mic Analysis  There are further, qualitative differences that I think are worth consideration.   Firstly, the unmatched contact mic bristles with a little something not present on the other channel. As with the epoxy resin comparisons I think this is related to the resonant frequencies of the contact mic itself and I think that these are attenuated by the impedance matching. Whilst that little bit of extra energy might be desirable in certain situations (I’m primarily thinking noise-music type applications) if you’re after more ‘audiophile’ results it’s worth investing in an impedance matching adaptor.  Secondly, the impedance matching transformer reduces the overall recording amplitude slightly. The tracks were normalised but no further edits/processing was undertaken. If I boost the volume on the 2nd channel the noise floor comes up with it (and apparently Zoom H4s can be a bit noisy anyway). This isn’t really a problem. When recording using impedance matching transformers we could raise the input sensitivity so the recording wouldn’t be quieter and it wouldn’t need a boost which brings the noise floor with it. As much as the slight ‘bristling’ mentioned above is possibly linked to resonance, I think the recording volume differences also play a small part. The moral of the story here is simply that I could have gotten a slightly better recording via the matched channel if it wasn’t for the controlled variables of the A/B comparison.That’s it really, these strike me as the most significant comparisons/differences. Again, I’m quite pleased as to how well an epoxy-backed, unmatched contact mic stacks up in general and would recommend anyone without the budget to invest in more expensive pro-mic’s and additional equipment to get stuck in and have a go. I’m also pleased with how well an impedance matching adaptor complements a relatively cheap contact mic. I paid ~£16.50 for the adaptor which, considering how much use I’m likely to get out of it, seems pretty good value. I’ll probably get a second one at some point so I can use them on both channels on my audio recorder or even use them for live work. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/impedance-matched-contact-mics/",
        "teaser":"http://localhost:4000/assets/images/contact-imp-ab-500x300.jpg"},{
        "title": "Little Arithmetics",
        "excerpt":"I’m hoping to try a little experiment. Things have been quiet here as I’ve temporarily relocated to Berlin and have been sorting things out and getting set up. I’m also hoping to start a PhD in October and with the last round of funding applications due any minute I’ve had other priorities. Still, I’m starting to make time again for reading, after a few indulgences with Ursula K Le Guin and William Gibson I’m on to some heavier, academic stuff. Rather than try to write any detailed critiques or appraisals of what I’m reading, for the time-being at least, I just want to pose small thoughts, provocations and responses. More of a thinking out loud than academic response. Hopefully this will allow for something a little briefer and more playful that I can return to in a more considered way at a later date. I’ve just started reading Joseph Nechvatal’s ‘Immersion Into Noise’ (2011) which, very early on contains the following Deleuze and Guattari quote (and which these thoughts will have nothing to say on beyond this quote):   What is real is the becoming itself, the block of becoming, not the supposedly fixed terms through which that which becomes passes. — Gilles Deleuze and Félix Guattari, A Thousand Plateaus I’d like to consider this in terms of Improvised Music Practice. I’m going to leave that term wide open in terms of types, idioms, non-idioms, genres, styles etc. of Improvised Music Practice and all the other attendant forms of labor and participation (audience, electronics DIYer, experimental instrument builder, promoter etc.) Instead I’d like to suggest a commonality. Forms of music practice centred around spontaneity of creation. I’m wary of a view of creativity that suggest thoughts put into practice so I will simply generalise this as a gestural process of doing (this may later have problems but it’s an abstraction for the purposes of my immediate thinking). Coming back to the above quote perhaps we can bootstrap this generalisation as creative activity that centres becoming rather than the things that have become. Long standing questions of the role of recorded artefacts of improvisation add challenge to this rationale but I would posit that these documents, when employed within improvised music practice are frequently an extension of processes which centre becoming rather than replacing these processes with a renewed focus on what has become (the role these may go to play in defining improvised music history and practice not withstanding). Why then do we bother with distinctions between forms of improvised music practice? From Jazz improvisation to Free Improv, improvised harsh noise to the No Audience Underground? My thinking is simply dissatisfaction. Critical engagement with the boundaries that calcify around such practices. Free Improvisers such as members of AMM were deeply influenced by Jazz musicians but recognised that, not being Black Americans, the boundaries around Jazz didn’t fit a white European perspective (and without attending to such problematics would easily reinforce forms of colonial power and cultural appropriation). Likewise, Free Improvisation, via characters such as Cardew and The Scratch Orchestra has a lot of relevance to challenging compositional authority imposed upon musicians within a Western Art Music tradition and various political discourses of the time. Whilst there are huge overlaps, Free Improv has less to say about the non-hierarchical and participatory social functions of the No Audience Underground (rather than the non-hierarchical and participatory aspects of art music ensembles). Whilst there is little that goes on at an average NAU show organised in the back room of a pub or a practice space in Sheffield, Leeds, Manchester etc. that differs significantly from the histories of Fluxus, free improv and psychedelic rock, they often feel a far cry away from the somewhat poe-faced arts council funded and culturally legitimised world of Free Improv shows at an arts centre or part of a Jazz festival. The fact the NAU shows take place primarily within a welcoming and open yet familial and insular social context is a significant differentiation. It feels somewhat naive to say but it’s for the ‘people’ more that for the ‘culture’ - yet at the same time there is a recognition that culture is something you make for yourselves rather than wait for someone else to make for you. I’m really not suggesting any of these forms are radically different from each other yet there is a radical praxis in critiquing and developing different forms of or social environments for improvised music practice. What I want to suggest then is that this radical praxis is itself part of the centring of becoming within improvised music practice. A part of the ‘doing’ of improvisation. Surely similar forces are at play in various sub-cultural shifts and genre fractures throughout forms of popular, classical, art and academic music (not to mention other cultural forms, scientific branches and philosophical ‘turns’). It’s not so much a case of saying improvised music is radically different, than a case of saying that such radical praxis is foregrounded by the nature of improvised music practice itself. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/little-arithmetics/",
        "teaser":"http://localhost:4000/assets/images/little-arithmetics-500x300.jpg"},{
        "title": "WNNS Review: Dissolution Matrix in Afterthought of Skiess",
        "excerpt":"I just wanted to share this most welcome review, from the ever-excellent We Need No Swords, of “Dissolution Matrix in Afterthought of Skies”   “Here’s something I’ve been sitting on for a while.” That was the message accompanying this collection of scruffy, welcoming improvisations from Murray Royston-Ward when the CD-r popped through my letterbox back in January. Well, its been sitting on my listening pile for a fair time too, with regular spins on on the WNNS hi-fi only emphasising its enduring appeal. And while Royston-Ward’s aesthetic sits squarely within the no-audience underground palette of abstract scrapes and splutters, the airy openness of the six tracks on offer here give listeners plenty to get their brains round. Royston-Ward’s ability to alchemise a familiar mix of field recordings, object clatter and free improvisation into something beguiling and intimate  is a pleasure to behold – witness the wibbles and mutters of ‘Images In Hair’, evoking a mellow afternoon sharing tea and smokes with visiting aliens. But my favourite moments are when Royston-Ward languidly picks apart avant-rock forms, as on cuts like ‘Waterpark’. The slow clang of detuned guitar and meandering, zero-gravity bass hint at the sprawl of Jim O’Rourke-era Sonic Youth, but with the arty opacity of those records switched out for beatific drift that repays attention with maximum wiry reward. Thurston – give Murray a call. He’ll get your career back on track. https://weneednoswords.wordpress.com/2018/07/21/murray-royston-ward-dissolution-matrix-in-afterthought-of-skies/ If anyone wants to listen for themselves it’s available through a variety of options: Firstly, it’s self-released by The House Organ and can be torrented freely and legally. The House Organ is also available via Tor Hidden Services tho2f4fceyghjl6s.onion and via the p2p DAT protocol dat:thehouseorgan.hashbase.io There is the co-operative streaming service Resonate. And of course there is Bandcamp: Dissolution Matrix in Afterthought of Skies by Murray Royston-Ward","categories": [],
        "tags": [],
        "url": "http://localhost:4000/wnns-review/",
        "teaser":"http://localhost:4000/assets/images/teaser500x300.jpg"},{
        "title": "Non-anonymous Tor Hidden Service using Google Cloud",
        "excerpt":"I’ve previously talked about how to set up Tor Hidden Services, at home on a Raspberry Pi. As I’ll be moving around soon and not have a permanent base to run such a mini-server I have been thinking about what to do. One option would be to take it with me and run it as a portable, nomadic server. This is appealing on several fronts but not practical for me right now as under some circumstances encryption technology and the use of Tor is highly scrutinised and problematic. Whilst I’m simply self-hosting and publishing my own music, therefore acting perfectly legally, not everywhere in the world sees it that way and unwanted scrutiny might jeopardise the projects of others I will be associated with. This is all probably a little extreme but it’s not a risk I’m currently able to take. Basically I’m going to temporarily be in a political context where I’ll very much have to toe the line for personal reasons. Another option would be to have a friend or family member take on the little server. Whilst this is feasible it’s not what I want to do right now. Especially because having to ring someone up and ask them to reboot it if there’s a problem is fine if it works first time but if troubleshooting becomes more difficult, it can become a burden. A third option would be to simply cease running the hidden service for a while. Not what I want to do. Finally then, I need to move from the small home-based server to another hosting solution. I could rent server space, set up a VPS etc. or, and I’m a curious fan of this at the moment, play with so-called ‘cloud’ solutions. This is problematic. My desire to publish independently in a way that critically engages with the economies of ‘platforms’ will now rely on the infrastructure of some of the largest and richest private companies in the world and be embedded with their numerous disturbing business practices and lines of money/power. I still have free trial credit with google though that I would like to use up and, at this stage, as a learning exercise, I feel that engagement with these platforms in such a way is beneficial. It helps me to develop technical skills and knowledge plus, by reporting this way, I can also help share this knowledge. Whilst ideally, at some point, I would move these services again, to somewhere less problematic, for the meantime it’s a simple and accessible solution to my problem. So… onwards. Setting up Google Compute Engine If you want to follow this guide you’ll need an account at https://cloud.google.com/. You can get free trial credits that, for low level usage such as this, should last for a year. Once we have an account we’re going to set up a new project. Call it whatever you want… for this example I’m using ‘tutorial’.         Creating a new project in Google Cloud  Now we’re going to choose ‘Compute Engine’ from the lefthand menu and ‘create’ a new ‘VM Instance’.         Creating a VM Instance in Google Compute Engine  Give it whatever name you want (I’m using ‘tutorial-instance’); choose your region (I opted for European data centres, I especially didn’t want to use US ones); machine type should just be ‘micro’ as we don’t need more power and it’s cheaper this way; boot disk should be Debian 9; on the firewall add HTTP and HTTPS access.         VM Instance settings in Google Compute Engine  That’s it, we now have a fresh Debian instance to work with. Google cloud has a really good web based SSH Terminal so click on that, allow pop-ups if they are blocked and initiate the connection.         Launching SSH Terminal in Google Compute Engine          SSH Terminal in Google Compute Engine  Installing Tor We need to run some commands in the terminal to get everything set-up and installed. First update the OS: $ sudo apt-get update$ sudo apt-get dist-upgradeNext we’re going to follow Tor instructions here - https://www.torproject.org/docs/debian.html.en We need to add package sources so create a new file in the correct folder: $ sudo nano /etc/apt/sources.list.d/tor.listPaste this into the new file nano has just created: deb https://deb.torproject.org/torproject.org stretch maindeb-src https://deb.torproject.org/torproject.org stretch mainctrl + x to save and quit. The package sources have now been added but things aren’t fully set up yet. First we need the apt-transport-https package to work with https urls. Next we’ll need dirmngr as it’s necessary when working with the gpg key. $ sudo apt-get install apt-transport-https$ sudo apt-get install dirmngrTo install Tor securely we also need to set-up the gpg key. I had trouble with this unless I ran the following commands (from the Tor instructions) as sudo: $ sudo gpg --keyserver keys.gnupg.net --recv A3C4F0F979CAA22CDBA8F512EE8CBC9E886DDD89$ sudo gpg --export A3C4F0F979CAA22CDBA8F512EE8CBC9E886DDD89 | sudo apt-key add -Now that’s all set-up we can run apt-get to install Tor as follows: $ sudo apt-get update$ sudo apt-get install tor deb.torproject.org-keyringThat’s it, Tor is installed. Installing nginx Next we’re going to set up a webserver. Apache isn’t recommended here simply because it bigger and more complex than we need. I’ve previously used lighttpd sucessfully but this time I am going to go with the commonly recommended nginx. First run: $ sudo apt-get install nginxAs I’m not running a site anonymously (I have public facing versions of the same site) I don’t need to worry about many of the warnings usually made about running hidden services. General server best practices should be adequate. A few settings we will still change with a view to minimising possible information leakage are as follows. Edit the file ‘/etc/nginx/nginx.conf’: $ sudo nano /etc/nginx/nginx.confIn the http section we’re going to set server_tokens to off and set some logging options like this (… means leave the other bits as they are and we’re pretty much disabling logging): http {...        server_tokens off;...        ##        # Logging Settings        ##        #access_log /var/log/nginx/access.log;        #error_log /var/log/nginx/error.log;        error_log /dev/null crit;ctrl + x to save and quit. That’s the basic set-up for nginx and it should be up and running. You can try by visiting your instance’s public IP although only HTTP will work so check the address bar if you have connection issues.         IP Address in Google Compute Engine  You’ll only see a very basic nginx landing page but at least we know it is working.         NGINX Welcome Page  The sites I use this for are publicly hosted using other services (such as gitlab pages) and the Tor version is a separate mirror. This might not be the case for you and, for non-anonymous sites, you may want to serve both publicly AND over the Tor network from this install. The next steps aren’t entirely necessary or correct if that is the case. The set-up for Tor will be near identical but you’ll want to set-up nginx to work as a regular web server too. In my case though I DON’T want this nginx install to be public facing at all. To achieve this I’m going to change settings so that nginx only serves via a locked down port from local connections (that Tor can still access). You’ll need to edit /etc/nginx/sites-available/default $ sudo nano /etc/nginx/sites-available/defaultAll you need is as follows so you can safely delete the other section and comments. We’re telling nginx to only listen on port 8080 from local connections and to block any other connections. We’re setting the root directory where our website files will live and we’re defining valid index pages. server {        listen 127.0.0.1:8080 default_server;        server_name localhost;        root /var/www/html/;        index index.html index.htm;        location / {                allow 127.0.0.1;                deny all;        }}ctrl + x to save and quit. Next we must restart nginx: $ sudo service nginx restartIf you refresh the web page it should no longer work as we can’t access the web-server other than locally (which Tor will be able to access and proxy over hidden service requests). Configuring Tor The final piece of the puzzle is to configure a Tor Hidden Service. $ sudo nano /etc/tor/torrcUncomment the two lines below and change the port settings as below. The first 80 is standard webpage port, you could change this but would then need to specify a port when browsing an onion address so 80 is standard. 127.0.0.1 is localhost and matches our nginx settings from earlier (remember we blocked access from anything other than local connections). :8080 is the port we specified in nginx set-up. We could use any different port to 8080 but the nginx and Tor config must match. HiddenServiceDir /var/lib/tor/hidden_service/HiddenServicePort 80 127.0.0.1:8080ctrl + x to save and quit and then restart Tor. $ sudo service tor restartTor will now generate the onion address for this hidden service and we can see it with the following command: $ sudo cat /var/lib/tor/hidden_service/hostnameIn my case it generated ‘arpf35uo2uscuxso.onion’. We need to use the Tor browser to visit onion addresses so open it up and visit your onion address. You should be getting an error page. When we set up nginx we specified some valid index pages. The default nginx debian index page is actually ‘index.nginx-debian.html’ which falls outside of the scope of our project. We can fix this by making a web-page called index.html. $ sudo nano /var/www/html/index.htmlType the following: &lt;h1&gt;Hello World&lt;/h1&gt;ctrl + x to save and quit. Now refresh Tor browser and you should see your new web page. In principle that’s it. You can put your web files in the /var/www/html/, nginx will serve them locally like a regular website, and Tor will proxy it to the onion address accessible via Tor browser. Taking Things Further This is great and relatively simple. It doesn’t quite cut it for me though. I want to host multiple sites so will need to set up ‘virtual hosts’ in nginx. The House Organ’s website is built using jekyll and uploading the generated site files to this Compute Engine every-time I make an update is problematic. In fact, transferring files to the site folder isn’t entirely straightforward under any circumstances. There are solutions to all of these problems and I will address them in future posts. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/non-anonymous-tor-hidden-service-using-google-cloud/",
        "teaser":"http://localhost:4000/assets/images/high-tor-500x300.jpg"},{
        "title": "Pushing Jekyll to a Google Cloud based Tor Hidden Service using Gitlab CI",
        "excerpt":"In my previous post I migrated from a Raspberry Pi home server to Google Compute Engine for the purpose of hosting Tor Hidden Service versions of my websites. This was primarily as I will temporarily not be able to run small home servers rather than due to any particular limitations of a home based setup. In my case the main project effected by this is thehouseorgan.xyz and its onion version tho2f4fceyghjl6s.onion. The House Organ is built with the static site generator Jekyll. I won’t be covering how to set this up or use it as there are ample tutorials. Also, the website code for The House Organ is freely available to review and reuse. Basically though, I add/modify content, run Jekyll and the site is rebuilt with the changes. It’s all very automatic and straightforward. The way the site is published is using Gitlab’s Pages functionality. I have a git repository set up. Whenever I update the site and run Jekyll I then push the changes to Gitlab. Gitlab pages can be set-up via their instructions, it’s fairly straightforward but does have a couple of hoops to jump through. Basically it uses Continuous Integration to automagically copy the generated site files within Gitlab’s backend infrastructure to the correct place for Gitlab pages to serve the site. Once the config file and Gitlab Runner are set-up everything runs pretty smoothly. I mention all of this because, in the previous post, getting the generated site files into the webroot folder on Google Compute Engine is quite difficult so I needed find a solution. Also, rather than manually copying files over every time the site is updated I wanted to try and automate this. This tutorial presumes that you are using a similar site generator, git and CI. It definitely works with Jekyll and Gitlab though there will be similarities and overlaps with other setups. Setting up Google Cloud Storage The biggest difficulty I found was finding an automated way to push the generated site folder to our GCE Instance. The solution was to set-up Google Cloud Storage. We create a storage bucket that we can easily push files to using CI; we mount the storage within the GCE virtual machine; and then we set-up nginx to use this bucket as the webroot and serve files from it. Setting up a bucket is fairly straightforward. In the Storage area click on ‘Create Bucket’.         Creating a Storage Bucket in Google Cloud          Creating a Storage Bucket in Google Cloud  It needs to have a unique name (I’ve gone with ‘tho-tutorial-bucket’), stick with multi-regional, and I preferred to use European data centres.         Creating a Storage Bucket in Google Cloud  Our VM Instance needs access via Cloud API access scopes to interact with the bucket so go to Compute Engine. You’ll need to ‘Stop’ the VM before you can ‘Edit’ the settings. Under ‘Access Scopes’ choose ‘Allow full access to all Cloud APIs’.         Editing Access Scopes in GCE VM Instance  Save and restart the VM. Once it’s running start up the SSH Terminal again. Mounting Google Cloud Storage Bucket in a GCE VM To mount our storage bucket we need to install gfuse. The following commands add the correct source repositories, update them and then install gcsfuse: $ export GCSFUSE_REPO=gcsfuse-`lsb_release -c -s`$ echo \"deb http://packages.cloud.google.com/apt $GCSFUSE_REPO main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list$ curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -$ sudo apt-get update$ sudo apt-get install gcsfuseWe also need a folder to mount to. Due to user permissions (and not wanting to use sudo as it can make mounting flakey) it seemed easiest to do this in the home directory so (where foo is whatever you want to call your folder): $ cd ~/$ mkdir fooBefore we mount there are a couple of things I learned the hard way. Nginx runs as user www-data. I couldn’t figure out any permissions situation that allowed nginx to access this bucket. The only option was to run gfuse with -o allow_other set which does have security implications (any user within our instance would have access to the contents of the bucket - I don’t however see this as a major problem for me here considering the simplicity of the server setup and the lack of sensitive data in the bucket). The other flag that needs setting is --implicit-dirs so that subdirectories work as expected. Go and run (where ‘tho-tutorial-bucket’ is the name of your bucket and ‘~/foo’ is the mount directory you just created): $ gcsfuse -o allow_other --implicit-dirs tho-tutorial-bucket ~/fooThis should however fail as we need to edit /etc/fuse.conf and uncomment user_allow_other Edit /etc/fuse.conf: $ sudo nano /etc/fuse.confand remove the # from in front of user_allow_other. ctrl + x to save and quit. Now try mounting again: $ gcsfuse -o allow_other --implicit-dirs tho-tutorial-bucket ~/fooThis should now be successfully mounted. Updating nginx Configuration ~/foo isn’t a practical folder for nginx to serve from though so we’re going to symlink this to somewhere more practical. If you remember, our original nginx set-up looks for our site files in /var/www/html. We’re going to symlink to a new folder within /var/www/html. This also helps us as we may, in the future want to set up other site directories and host multiple sites using ‘virtual hosts’. Run the following (changing foo for whatever suits you best and whatever you chose earlier): $ sudo ln -s ~/foo /var/www/html/fooWe need to update our nginx configuration so that it looks in this new folder so edit /etc/nginx/sites-available/default again and change the webroot folder to /var/www/html/foo: $ sudo nano /etc/nginx/sites-available/defaultupdate this line: root /var/www/html/foo;ctrl + x to save and quit. We need to restart nginx for the changes to take effect. $ sudo service nginx restartAutomounting Cloud Storage Bucket Currently, if we stop and start the VM Instance again, we’ll have to log in and manually mount our cloud storage again. This is far from ideal but we can set-up automounting by editing /etc/fstab. $ sudo nano /etc/fstabAdd the following line at the bottom (we need the full path so you’ll need your username): tho-tutorial-bucket /home/username/foo gcsfuse rw,allow_other,implicit_dirsRight, there’s still nothing in this bucket for nginx to serve so we’re going to make a quick test file. $ sudo nano ~/foo/index.htmlWe’ll just make another simple Hello World page (but different to our previous one so we know the new file is being served). &lt;h1&gt;Hello Bucket World!&lt;/h1&gt;ctrl + x to save and quit. Now stop the VM Instance. Next start it up again. Now, back in Tor Browser navigate to your onion domain (mine was arpf35uo2uscuxso.onion) and refresh the page. Fingers crossed we’re now seeing our new page which means everything is set-up correctly. Viola! Just to note, there are now numerous options available for you to connect to this storage bucket and upload your website files. Personally I use cyberduck but your options are broad and wide. Setting Up CI/CD As previously mentioned, I’m going to base this next part of the tutorial upon the notion that you already have a Gitlab Pages Jekyll site. This is because it is my set-up. I initially set this site up with Github before migrating to Gitlab. When I did move from Github to Gitlab the big thing I had to figure out is how to use Gitlab’s Continuous Integration to deploy to ‘pages’. When I did figure it out (and I used scripts which I found online but didn’t document so I can’t share links), I had a .gitlab-ci.yml file that looked like this (if you don’t know what any of this means work through the tutorials): # requiring the environment of Ruby 2.3.ximage: ruby:2.3# add cache to 'vendor' for speeding up buildscache:  paths:    - vendor/before_script:  - bundle install --path vendor# add a job called 'test'test:  stage: test  script:    - bundle exec jekyll build -d test/  artifacts:    paths:      - test # generating site folder to be browsed or download  except:    - master # the 'test' job will affect all branches expect 'master'# the 'pages' job will deploy and build your site to the 'public' pathpages:  stage: deploy  script:    - bundle exec jekyll build -d public  artifacts:    paths:      - public  only:    - master # this job will affect only the 'master' branchBasically this fires up a Ruby environment and builds the site into a folder called ‘public’ which is where gitlab pages serves it from. I’ve never actually used the ‘test’ option but ho-hum it’s there anyway. The plan is then to expand this CI configuration file to also connect to our storage bucket and copy over the site files there so they are accessible by our Tor Hidden Service. Adding a few more commands was fairly straightforward. Firstly we’re using dpl (which is a travis deployment package) which can be installed as a gem gem install dpl. This can connect to numerous services. It can connect directly to a GCE VM Instance but not in the way that we need hence we’ve set up a storage bucket. We’re going to need access keys though so, back in the cloud storage settings, in Storage &gt; Settings &gt; Interoperability you’ll find your ‘Access Key’ and ‘Secret’.         Locating Access Keys in Google Cloud Storage  We need to be careful with these keys and keep them secret. We therefore can’t use them directly in our CI config otherwise anyone could see them by looking at our public code. Rather, we’re going to use Gitlab’s CI variables (presuming you’re using Gitlab Pages like me). Within your Gitlab Project navigate to Settings &gt; CI / CD.         Gitlab CI / CD Settings  Then expand the section ‘Variables’. We need to create 2 new variables called ACCESS_KEY_ID and SECRET_ACCESS_KEY_ID. Use the ‘Access Key’ and ‘Secret’ from earlier.         Setting Gitlab CI / CD Variables  Now, when we add to our CI config we can reference these variables without revealing their values to the public. To use dpl we string together the following:   provider is gcs (google cloud storage)  access-key-id is the $ACCESS_KEY_ID variable we set earlier  secret-access-key is the $SECRET_ACCESS_KEY_ID variable  bucket is tho-tutorial-bucket (or whatever you set previously)  local-dir is the jekyll _site folder where the built site is stored. It’s important to use this folder (rather than try and use the public folder the CI runner has previously built to) as it’s part of the git project and dpl uses this (it’s primarily designed for pushing the whole git project I guess).The entire string looks like: dpl --provider=gcs --access-key-id=$ACCESS_KEY_ID --secret-access-key=$SECRET_ACCESS_KEY_ID --bucket=tho-tutorial-bucket --local-dir=_siteEdit the ‘pages’ section of your .gitlab-ci.yml like this: pages:  stage: deploy  script:    - bundle exec jekyll build -d public    - gem install dpl    - dpl --provider=gcs --access-key-id=$ACCESS_KEY_ID --secret-access-key=$SECRET_ACCESS_KEY_ID --bucket=tho-bucket --local-dir=_site  artifacts:    paths:      - public  only:    - masterNow, whenever you commit a site change (i.e. a new post) it triggers the usual gitlab pages build process and puts your site into the ‘public’ folder and it also connects to your cloud storage bucket and puts your ‘_site’ directory there which is then served internally by nginx and proxied as a hidden service by Tor. A note here though. You must make sure you have built or served the changes locally before committing/deploying so that the committed _site folder is up-to-date. This is usually fine if you’re working on changes on your computer and checking them in the local server before committing. If you don’t do this the _site folder won’t be updated with your changes. You can build within the CI runner but if the changes aren’t committed to git then they won’t upload to gcs and the hidden service site won’t reflect the changes on the gitlab pages site. I felt this was a small price considering I usually update the site this way anyway and I didn’t want to go into trying to figure it out any deeper. Further Developments I’ve pretty much managed to migrate the onion version of thehouseorgan.xyz from a home Raspberry Pi server to Google’s Cloud platform following the steps in this and the previous tutorial. The only things that are missing from this set-up are firstly that I need to set up ‘virtual hosts’ so I can serve more than one website this way. Secondly, I also had a torrent client set-up with all of the torrents of my music which I make available. This basically acts like an always on seed so I don’t have to leave my laptop running. I’ll need to try and set this up on GCE too. ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/pushing-jekyll-to-a-google-cloud-based-tor-hidden-service-using-gitlab-ci/",
        "teaser":"http://localhost:4000/assets/images/funnel-clouds-500x300.jpg"},{
        "title": "An (Opinionated) Critical Guide to DIY Digital Publishing",
        "excerpt":"Introduction I’m briefly introducing this text as it is taken directly from a supplement I recently published with TQ#15. It was quite spur of the moment, having wanted to try and collect some of these thoughts for a while now, so I contacted Andy and he was up for it and it all came together quite quickly. It’s likely to be out-of-date quite soon, but it does represent a current snapshot, of the problematic flows of power and capital that run through our practices of recording and releasing music on-line.  Here is a PDF of the supplement,  or an ePub ebook and below is the text itself. Releasing Music in the NAU This international music community we call the No Audience Underground (NAU)1 thrives upon small-scale publishing. From micro-labels dropping cash on short runs of vinyl, to home burned 3” CDrs. Rather than opposite ends these are part of a continuum, also embracing tape-trading, mail art, craft forms, and co-operative organisation. I have avoided mentioning digital, but artists have varied widely from downright avoiding digital formats and social media, to leading the charge with download codes, streaming platforms, and online communities. What I feel is true, of both physical and digital forms of production within the NAU, is that there’s ZERO cynical ‘business’ only operators. People care about what they are doing and make choices that best fit contributing positively within the scene. That doesn’t exclude anyone from wanting to turn a profit – though it’s uncommon – but it’s generally of the, “so I can fund more releases and/or make this my day job,” variety. The general path most of us follow, in putting some music out into the world, means that we will likely brush with DIY2 publishing. For most the online/digital cat is well and truly out of the bag – refusing it completely is simply not a viable option and we must consider digital publishing and distribution of our works. Some stubbornly follow/disrupt format trends, package releases as totemic objects, make things cheap and basic, use recycled and ethical materials, expand notions of formats through creative combinations of downloads and physical things. These choices are somewhat political and the majority of fellow travellers I have met try to live and act their varied politics within their work. The Politics of Digital Publishing Many of us have been involved with peer-to-peer (p2p) file-sharing via e.g. Soulseek and Torrents. This is nothing new. My dad used to order records in from the library and copy them to cassette. Him and his work mates would lend music to one another for the same purposes3. The digital equivalent has led to shifts, even within our small corner, and there is a tension between feeling like our digital works have little to no value and wanting music to circulate freely. How can something with a lack of physical tangibility, that can be instantly and perfectly re-produced, be worth anything? How can the output of our creative labour not be worth anything? Lately p2p file sharing feels like less of an anti-corporate protest than gluttonous gorging. But let’s not forget the economies at play, especially with the everlasting aftershocks of neoliberal global economics, never-ending financial crises, ‘austerity’, and imminent #Brexitbus armageddon. Many of us can’t afford all of the nice things4 and sharing music amongst ourselves keeps ours, and our community’s, wellbeing loosely in check. Snowden’s revelations highlighted the scale of state and corporate surveillance under which we are almost constantly subjected. Social media and advertising tracking cookies silently build shadowy profiles of our interests and engagements. Cambridge Analytica and Russian state interference exploit this to manipulate Facebook advertisers who remain complicit5. Similarly, electoral law is broken by the Brexit leave campaigns and we seem powerless to hold anyone to account6. What emerges is a lot of power held in very few corporate hands, and especially in the hands of GAFAM (Google, Apple, Facebook, Amazon and Microsoft), within so-called ‘Platform Capitalism’. As individuals our clicks, our time, our engagement, and our content is monetised, frequently with little-to-no personal remuneration or benefit (we instead simply get access to the very services which are preying on us). These same forces shape our DIY online publishing. As ‘content producers’ we utilise platforms to try and reach audiences, yet our content and interaction is monetised by the platform. We’re not making money from our junk clatter, field recording collages, drone-athons, and other deep excavations. Perhaps we don’t expect to make money anyway, so it doesn’t really matter. The pleasure of getting a random £4 on Bandcamp, the communities that we have formed, the trades facilitated, the gigs we found out about etc. far outweighs most of these negatives. How much do you really think GAFAM are making off of our tiny little pocket of the web? The flip side is that we tolerate it because we have little choice. Yes, we can opt out (and many of us do refuse Facebook, take social media breaks, try and use alternatives etc.) but it’s not as easy a choice as that suggests. It comes at a significant price, especially if trying to share music within a community such as ours7. The logic of opting out disproportionately favours the already established over newer voices. How many of those newer voices have formerly been marginalised voices that will remain largely unheard if the only option is to opt-out? The idea we can solve these problems through market choice is a dead end – falling short of actually addressing the underlying power structures. If it’s not about the money and we haven’t really got much choice, then what’s the beef? For me, specifically within our context, it’s about hegemony and power dynamics. We express a vibrant diversity across releases and approaches to publishing, yet frequently the solution to putting our work online is to put it on Bandcamp. I say this not to shit on Bandcamp (or us for doing so), but to highlight the one-size-fits-all solution that dominates most independent music communities. Bandcamp is great, it works brilliantly, payment seems fair, we’re not bombarded with adverts. I put my stuff on Bandcamp and it works really well for me. Bandcamp is great. Some of us try and get our music on Google Play, Amazon, iTunes and Spotify. There are large hoops to jump through and costs associated with maintaining a catalogue with these services (via 3rd Party distribution). Costs and processes that fit much better with record labels (even small ones) than DIY, personal publishing. The remuneration rates are terrible so, for most of us, this will always be a loss leader8. iTunes, Spotify, Amazon and Google assume a role not dissimilar to the Major record labels. Bandcamp is more of an Independent. What’s worrying is that they seem to be pretty much the only Indy in the race9. That’s a lot of trust and power put in the hands of one organisation. The hosting and buying of our music is only part of the publishing paradigm and again, much of this is dominated by Facebook Pages, Google Blogs and a handful of other social media platforms. I could make largely the same arguments about these services but shall refrain from repeating myself. The Monoculture of Dominant Platforms This lack of platform diversity also reinforces dominant monetisation, corporate and technological strategies. Spotify relies on subscription payments. Apple, Google, Amazon and Soundcloud offer similar services. Apple, Google and Amazon also offer traditional pay to download, with prices being mostly fixed and favourable towards the platform. Bandcamp is basically a similar marketplace where the rates for artists are much more reasonable and pricing is super flexible. All of these are organised as hierarchical, for-profit, corporate entities. Most come with subtle licensing restrictions10. The ongoing state and corporate surveillance, referenced earlier, has led to far more public interest in using browser plugins and technological enhancements to resist corporate, digital profiling, and improve personal privacy. Subscription services can be hostile towards the use of Virtual Private Networks (VPN)11. None of the afore-mentioned music platforms are available via Tor Hidden Services, which are used by privacy advocates globally12. None of the services offer any p2p network technology (such as p2p browser protocols or mesh networks), meaning that in the extreme case of hostile state shutdown or government censorship they’d be done and dusted. Some of these examples are getting extreme just for sharing 20 mins of dictaphonix vocal jaxx with ~30 fellow moong bean enthusiasts. These kinds of end-of-the-world scenarios come out not because of a lack of perspective, but because this is all intertwined with much larger debates concerning intellectual freedom, the need for an open internet, and risks of authoritarian power structures13. Despite our variety of approaches to putting out hand-stamped CDrs, one-off lathe-cuts, wooden boxes made from discarded furniture; despite our numerous takes on sharing, trading, collaborative mail-art, selling from the car boot after a show, sneaking tapes into charity shops; despite all of this, digitally we use a handful of dominant platforms, pay via one of a handful of dominant payment processors and download a file directly from a centralised server system – all mediated by a handful of corporate behemoths. You could liken this to actual shops – they’re certainly dominated by a handful of large corporations – but even large chains such as the Co-op and John Lewis have alternative corporate ownership structures. Where are the alternatively structured music platforms, where’s the diversity, where’s the resistance to Neoliberal Capitalist Markets? As the general shape of this debate has been running for quite a few years there are, and have been, a number of alternatives. It’s telling that many of the more viable ones generally come from a librarianship/archiving perspective (archive.org and freemusicarchive.org in particular), and whilst these easily facilitate the upload and sharing of your works, they are not really tools to promote, sell or market. They are repositories and not ‘publishing’ tools. A current alternative is co-operatively owned streaming platform Resonate (https://resonate.is) which is free to put your music on. You pay a small amount each time you listen to a track (loading up credits to cover your listening), and by the time you’ve played a track 9 times you’ve paid the equivalent of just buying it and so you now own it. So far, I’m a fan but it’s early days and still needs a bit of work. It’s not clear whether the above system will work or be anything more than a curiosity in a few years time. There are probably even better ideas out there and facets of the debate to which we are currently blind. My hope is that we take the time to explore them, remain open, and attempt the same levels of diversity in our online choices. Before I get to my final position I’d like to make it abundantly clear that I don’t necessarily advocate ditching all of these dominant platforms. My own personal engagement has definitely mellowed over the last year. After ditching Bandcamp completely I came to regret it, have re-engaged again, and managed to sell a handful of CDs to some of you in the process. If it works for you use it. You don’t need me to tell you that, but it’s a message that often gets lost in this debate. I think that there are larger battles to fight and, if that’s your bag, you should consider supporting the work of organisations tackling state/corporate surveillance, and its related forms of power/exploitation, head on14. If You’re Going to Do It Yourself You Should Really Do It Yourself, or Together There are some other alternatives than can, and I would suggest should, be run alongside our use of the aforementioned platforms. There are many aspects of these services that we can run and create ourselves. This generally involves acquiring technical knowledge, broadly categorised within the computer sciences, such as programming, server administration and web development. These are seen as high-level skills that require significant investment of time and training to achieve. I would suggest a far more amateur, DIY approach to these topics. Just as many of us learned a basic modicum of HTML to customise an online profile/template, the basics of running and maintaining simple web services ourselves is easily accessible. Speaking from my own experiences, I have engaged with all of the practices outlined below, and mostly I’ve poked and prodded until I could get something working. I never trained in any of these areas but have since acquired a functional, if uneven, body of expertise. It is straightforward, and entirely possible, to run a small web-server from home, on a credit card sized computer called a Raspberry Pi, which costs ~£30. It is also possible to self-host an .onion version of this to harness the privacy enhancing features of the Tor Network. Whilst there are still issues with dominant platforms, one can also easily and cheaply (often staying within free usage plans) run services on Google or Amazon Cloud Platforms (you’ll learn valuable skills along the way and define your own terms for publishing practices which can later be put to use with more ethical providers). If you begin to outgrow the above services there are far more choices, for Virtual Private Server providers, that won’t break the bank or your moral compass. There are static site generators, such as Jekyll and Hugo, that allow you to produce fully featured websites with no need of complex backend systems and database installations. They do require some configuration and setup but, once the initial barriers have been overcome, the skills learned pay dividends. Clubbing together to amortise costs and share resources could see a community offer a variety of services for little individual economic outlay. Or hook-up with a local group of internet privacy activists, get involved, volunteer some time and resources, and likewise get support with your DIY digital publishing using their servers/resources. There are p2p browsers which avoid the need for web-hosting altogether. DAT (which uses Beaker Browser) and ZeroNet are currently the most viable. The biggest drawback is that if you turn your computer off the site will be unavailable but, the more connections you make the more your site’s hosting will be spread around your community and this would cease to become an issue. It’s easy to make a torrent file of your own work and post this online, and you don’t need to engage with problematic pirate websites15. The benefits of legal torrenting within a small community seems very much inline with many of our offline practices. I haven’t done any of this in a vacuum and neither should you: I have learned from online communities, tutorials and more knowledgeable others along the way, and likewise I try to share what knowledge I have16. Anyone is free to contact me if they need any help or advice17. I also keep banging away at some of these ideas with The House Organ - https://thehouseorgan.xyz1819202122 I fully understand that many of us struggle with technology or are spread too thinly to even think about building alternative systems. Frequently these barriers lead to a certain fatalism regarding the whole subject. My biggest hope in writing this is that it might facilitate more of a debate within our community about such issues, and connect some people already working within this landscape. There are a number of aspects, regarding the current status quo of online DIY publishing, that seem counter to much of our politics and existing working practices, and I hope that collectively we might begin to approach building our own solutions. Who wants to explore these alternatives? Perhaps we can set-up some interest groups and/or resource sharing practices? Ultimately, I hope that our DIY practices expand more deeply into the way we publish and share our music digitally. Erm… this is quite a one-sided format for such a discussion. Somebody else… Anybody… Say something… Thanks This work is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0) Kevin Sanders is always a source of critical thinking across these issues – i.e. his work with Radical Librarians Collective where I’d also name check Simon Bowie. Andy Wood/TQ for supporting my work and giving me a platform for this supplement, and Isaac Ray for editing advice.             https://radiofreemidwich.wordpress.com/2015/06/14/what-i-mean-by-the-term-no-audience-underground-2015-remix/ &#8617;               I’m using DIY here as a proxy for the whole spectrum of Do It Yourself, Do It Together, self-releasing and small independent labels. &#8617;               Anecdotally they were industrial workers from the Midlands so I grew up with a lot of Heavy Metal and, despite an often scholarly rhetoric around intellectual freedom, I always associate file-sharing etc. with working class perspectives and the shop floor. &#8617;               Though let’s keep this in perspective considering the harm and violence of public cuts upon the vulnerable. &#8617;               I’m sticking here with UK news but there’s evidence of WhatsApp being similarly weaponised across the global south - https://tacticaltech.org/news/release-odos-whatsapp/ &#8617;               i.e. Investigative journalism of Carole Cadwalladr if you’re in any doubt as to any of these claims - https://www.theguardian.com/profile/carolecadwalladr &#8617;               I speak from experience here having destroyed the majority of my online and social media presence about 3 years ago, only really coming back within the last year fully, and noticing a significant difference in my ability to reach a small audience. &#8617;               Damon Krukowski provides an informative roundup of this landscape with the example of how some of the Galaxie500 back catalogue fares (and if they’re done badly by this system we haven’t a hope in hell) https://pitchfork.com/features/oped/how-to-be-a-responsible-music-fan-in-the-age-of-streaming/ &#8617;               Okay there are a few other services such as Tidal, Deezer, Pandora etc. but they fill small market niches within exactly the same structures as the afore-mentioned services. &#8617;               e.g. marginalisation which CC works may face by the distribution companies acting as gateways to major platforms - https://jelsonic.com/royalty-free/the-distros-dont-want-your-creative-commons-music/ &#8617;               Ostensibly due to geographic licensing restrictions and currently more prominent with video services i.e. Netflix. &#8617;               As a proof of concept I onionised Bandcamp though you have to jump through a lot of hoops to use it - https://mroystonward.github.io/onionised-bandcamp/ &#8617;               A great round up of digital privacy risks, and steps we can take to mitigate, comes from Extraction Music poster boy Kevin Sanders -  https://rightsinfo.org/digital-privacy-protect/ &#8617;               e.g. Open Rights Group (https://openrightsgroup.org), Privacy International (https://privacyinternational.org), Electronic Frontier Foundation (https://eff.org) and Tor (https://torproject.org) &#8617;               The first time I did this I followed this guide - https://lifehacker.com/5534190/how-to-share-your-own-files-using-bittorrent &#8617;               I’ve been posting some tutorials at mroystonward.github.io &#8617;               Twitter (@mroystonward) / E-mail (murray.s.roystonward@posteo.net) &#8617;               tho2f4fceyghjl6s.onion &#8617;               dat:thehouseorgan.hashbase.io &#8617;               https://thehouseorgan.bandcamp.com &#8617;               https://resonate.is/catalog/label/6151/The_House_Organ &#8617;               Source-code available at https://gitlab.com/thehouseorgan/thehouseorgan.gitlab.io &#8617;       ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/an-opinionated-critical-guide-to-diy-digital-publishing/",
        "teaser":"http://localhost:4000/assets/images/diy-digital-500x300.png"}]
